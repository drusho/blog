{
  
    
        "post0": {
            "title": "Web Scrapping Popular Youtube Tech Channels with Selenium",
            "content": "Notebook Created by: David Rusho (Github Blog | Tableau | Linkedin) . About the Data . Web scraping was performed on the Top 10 Tech Channels on Youtube using Selenium (an automated browser (driver) controlled using python, which is often used in web scraping and web testing). These channels were selected using a Top 10 Tech Youtubers list from blog.bit.ai. . Data from 2,000 videos was scrapped, which equals about 200 of most popular videos per channel. . Introduction . Collecting and Cleaning Data . Web Scrapping Youtube Channels . import pandas as pd import time from selenium import webdriver from selenium.webdriver.common.keys import Keys # Chrome driver location (for M1 macbook air) DRIVER_PATH = &quot;/opt/homebrew/bin/chromedriver&quot; # activate driver driver = webdriver.Chrome(executable_path=DRIVER_PATH) # Scroll to bottom of page def scroll_page(): for x in range(7): html = driver.find_element_by_tag_name(&quot;html&quot;) html.send_keys(Keys.END) time.sleep(2) def scrap_videos(): scroll_page() chan_xpath = &#39;//*[@id=&quot;channel-name&quot;]&#39; subs_xpath = &#39;//*[@id=&quot;subscriber-count&quot;]&#39; videos_class = &quot;style-scope ytd-grid-video-renderer&quot; views_xpath = &#39;.//*[@id=&quot;metadata-line&quot;]/span[1]&#39; post_date_xpath = &#39;.//*[@id=&quot;metadata-line&quot;]/span[2]&#39; title_xpath = &#39;.//*[@id=&quot;video-title&quot;]&#39; # Scrap Channel Name try: channel_name = driver.find_element_by_xpath(chan_xpath).text except (Exception,): pass # Scrap Number of Subscribers try: subscribers = driver.find_element_by_xpath(subs_xpath).text except (Exception,): pass # Reassign variable to recalculate all videos videos = driver.find_elements_by_class_name(videos_class) # Loop through all videos for video in videos: # grab title if available try: title = video.find_element_by_xpath(title_xpath).text except (Exception,): pass # grab url if available try: url = video.find_element_by_xpath(title_xpath).get_attribute(&quot;href&quot;) except (Exception,): pass # grab views if available try: views = video.find_element_by_xpath(views_xpath).text except (Exception,): pass # grab post date if available try: post_date = video.find_element_by_xpath(post_date_xpath).text except (Exception,): pass video_items = { &quot;channel_name&quot;: channel_name, &quot;subscribers&quot;: subscribers, &quot;title&quot;: title, &quot;views&quot;: views, &quot;post_date&quot;: post_date, &quot;url&quot;: url, } vid_list.append(video_items) return vid_list # scrap Channel About section def scrap_about(): chan_name_xp = &#39;//*[@id=&quot;channel-name&quot;]&#39; chan_join = &#39;.//*[@id=&quot;right-column&quot;]/yt-formatted-string[2]/span[2]&#39; chan_views = &#39;.//*[@id=&quot;right-column&quot;]/yt-formatted-string[3]&#39; chan_desc = &#39;.//*[@id=&quot;description&quot;]&#39; # Scrap Channel Name try: channel_name = driver.find_element_by_xpath(chan_name_xp).text except (Exception,): pass # Scrap Channel Join Date (about) try: channel_join = driver.find_element_by_xpath(chan_join).text except (Exception,): pass # Scrap Channel Views (about) try: channel_views = driver.find_element_by_xpath(chan_views).text except (Exception,): pass # Scrap Channel Description (about) try: channel_description = driver.find_element_by_xpath(chan_desc).text except (Exception,): pass about_items = { &quot;channel_name&quot;: channel_name, &quot;channel_join_date&quot;: channel_join, &quot;channel_views&quot;: channel_views, &quot;channel_description&quot;: channel_description, } vid_list.append(about_items) return vid_list # top youtubers based off &#39;https://blog.bit.ai&#39; top_youtubers = [ &quot;ijustine&quot;, &quot;AndroidAuthority&quot;, &quot;Mrwhosetheboss&quot;, &quot;TechnoBuffalo&quot;, &quot;TLD&quot;, &quot;austinevans&quot;, &quot;unboxtherapy&quot;, &quot;LinusTechTips&quot;, &quot;UrAvgConsumer&quot;, &quot;mkbhd&quot;, ] # empty list to hold video details vid_list = [] # url of most videos sorted by most popular for youtuber in top_youtubers: print(f&quot;processing {youtuber}&quot;) url = f&quot;https://www.youtube.com/{youtuber}/videos?view=0&amp;sort=p&amp;flow=grid&quot; driver.get(url) scroll_page() vid_list = scrap_videos() about_url = f&quot;https://www.youtube.com/{youtuber}/about&quot; about = driver.get(about_url) driver.implicitly_wait(10) about_items = scrap_about() # Close Chrome browser driver.quit() # create pandas df for video info df_channel = pd.DataFrame(vid_list) # export df to csv df_channel.to_csv(&quot;yt_channel_scrap.csv&quot;) . . Web Scrapping Youtube Videos . import pandas as pd import time from selenium.webdriver.chrome.options import Options from selenium.webdriver.support.wait import WebDriverWait from selenium.webdriver.support import expected_conditions as EC from selenium.webdriver.common.by import By from datetime import datetime from requests import options from selenium import webdriver # driver options (size and headless) options = Options() options.add_argument(&quot;--headless&quot;) options.add_argument(&quot;--window-size=1920x1080&quot;) # Chrome driver location (for M1 macbook air) DRIVER_PATH = &quot;/opt/homebrew/bin/chromedriver&quot; # activate driver driver = webdriver.Chrome(executable_path=DRIVER_PATH, options=options) driver.execute_script(&quot;window.scrollTo(0, document.body.scrollHeight);&quot;) # partial video description def par_description(): vid_desc = &quot;//div[@class=&#39;watch-main-col&#39;]/meta[@itemprop=&#39;description&#39;]&quot; elems = driver.find_elements_by_xpath(vid_desc) for elem in elems: return elem.get_attribute(&quot;content&quot;) # publish_date def publish(): pub_date = &quot;//div[@class=&#39;watch-main-col&#39;]/meta[@itemprop=&#39;datePublished&#39;]&quot; elems = driver.find_elements_by_xpath(pub_date) for elem in elems: return elem.get_attribute(&quot;content&quot;) # upload_date def upload(): upload_date = &quot;//div[@class=&#39;watch-main-col&#39;]/meta[@itemprop=&#39;uploadDate&#39;]&quot; elems = driver.find_elements_by_xpath(upload_date) for elem in elems: return elem.get_attribute(&quot;content&quot;) # genre def genre(): genre = &quot;//div[@class=&#39;watch-main-col&#39;]/meta[@itemprop=&#39;genre&#39;]&quot; elems = driver.find_elements_by_xpath(genre) for elem in elems: return elem.get_attribute(&quot;content&quot;) # video_width def width(): v_width = &quot;//div[@class=&#39;watch-main-col&#39;]/meta[@itemprop=&#39;width&#39;]&quot; elems = driver.find_elements_by_xpath(v_width) for elem in elems: return elem.get_attribute(&quot;content&quot;) # video_height def height(): v_height = &quot;//div[@class=&#39;watch-main-col&#39;]/meta[@itemprop=&#39;height&#39;]&quot; elems = driver.find_elements_by_xpath(v_height) for elem in elems: return elem.get_attribute(&quot;content&quot;) # Interaction Count def interactions(): interactions = &quot;//div[@class=&#39;watch-main-col&#39;]/meta[@itemprop=&#39;interactionCount&#39;]&quot; elems = driver.find_elements_by_xpath(interactions) for elem in elems: return elem.get_attribute(&quot;content&quot;) # Video_title def video_title(): video_title = &quot;//div[@class=&#39;watch-main-col&#39;]/meta[@itemprop=&#39;name&#39;]&quot; elems = driver.find_elements_by_xpath(video_title) for elem in elems: return elem.get_attribute(&quot;content&quot;) # Channel_name def channel_name(): channel_name = ( &quot;//div[@class=&#39;watch-main-col&#39;]/span[@itemprop=&#39;author&#39;]/link[@itemprop=&#39;name&#39;]&quot; ) elems = driver.find_elements_by_xpath(channel_name) for elem in elems: return elem.get_attribute(&quot;content&quot;) # Number Likes def likes(): likes_xpath = &quot;(//div[@id=&#39;top-level-buttons-computed&#39;]//*[contains(@aria-label,&#39; likes&#39;)])[last()]&quot; return driver.find_element_by_xpath(likes_xpath).text # Total Comments def comments(): # Move Page to display comments # set scroll pause time SCROLL_PAUSE_TIME = 0.5 # scroll to page bottom driver.execute_script(&quot;window.scrollTo(0, 1080)&quot;) # Wait for page load time.sleep(SCROLL_PAUSE_TIME) # scroll to page bottom driver.execute_script(&quot;window.scrollTo(300, 1080)&quot;) # Wait to load page time.sleep(SCROLL_PAUSE_TIME) com = WebDriverWait(driver, 10).until( EC.presence_of_element_located( (By.XPATH, &#39;//*[@id=&quot;count&quot;]/yt-formatted-string&#39;) ) ) return com.text # import csv of youtube channels data df_channels = pd.read_csv( &quot;yt_channel_scrap.csv&quot;, ) # new df of channel names and urls df_videos = df_channels[[&quot;channel_name&quot;, &quot;url&quot;]].dropna() # isolate video urls to a list url_list = df_videos.url.to_list() vid_list = [] url_fails_ls = [] count = 0 # # launch driver(s) for url in url_list: driver.get(url) count += 1 time.sleep(3) subscribe_button = &#39;//*[@id=&quot;subscribe-button&quot;]&#39; WebDriverWait(driver, 30).until( EC.presence_of_element_located((By.XPATH, subscribe_button)) ) try: comments_num = comments() likes_num = likes() chan_name = channel_name() v_duration = duration() p_description = par_description() publish_date = publish() upload_date = upload() v_genre = genre() v_width = width() v_height = height() title = video_title() interaction_count = interactions() except: print(f&quot;EXCEPTION RAISED for {url}&quot;) url_fails_ls.append(url) pass video_items = { &quot;url&quot;: url, # primary key &quot;Channel Name&quot;: chan_name, &quot;Title&quot;: title, &quot;Duration&quot;: v_duration, &quot;Partial Description&quot;: p_description, &quot;Publish Date&quot;: publish_date, &quot;Upload_date&quot;: upload_date, &quot;Genre&quot;: v_genre, &quot;Width&quot;: v_width, &quot;Height&quot;: v_height, &quot;Likes&quot;: likes_num, &quot;Comments&quot;: comments_num, &quot;Interaction Count&quot;: interaction_count, } vid_list.append(video_items) # print(f&quot;url {count} of {len(url_list)} complete&quot;) # print every 10th url if count % 10 == 0: print(f&quot;URL {count} of {len(url_list)} processed.&quot;) driver.quit() # # create dfs for video and failed urls df_videos = pd.DataFrame(vid_list) # store urls that failed to load in driver url_fails_dict = {&quot;url&quot;: url_fails_ls} df_url_fails = pd.DataFrame(url_fails_dict) print(&quot;Driver Quit&quot;) print(&quot;Code Duration: {}&quot;.format(end_time - start_time)) print(f&quot;Videos Processed: {len(vid_list)}&quot;) print(f&quot;Failures: {len(url_fails_ls)}&quot;) # export df to csv df_url_fails.to_csv( &quot;url_fails.csv&quot; ) df_videos.to_csv( &quot;yt_videos_scrap.csv&quot; ) . . Importing and Cleaning the Data . Note: Code in the cell below comes from this notebook I created to originally clean and merge the data. . import pandas as pd # load channel csv yt = pd.read_csv(&quot;yt_channel_scrap.csv&quot;, parse_dates=[&quot;channel_join_date&quot;]) # create df of Channel details channel_details = yt[yt.channel_join_date.notna()] channel_details = channel_details.drop( columns=[&quot;Unnamed: 0&quot;, &quot;subscribers&quot;, &quot;title&quot;, &quot;views&quot;, &quot;post_date&quot;] ).reset_index(drop=True) # create df Video details video_details = yt[yt.channel_join_date.isna()] video_details = video_details.drop( columns=[ &quot;Unnamed: 0&quot;, &quot;channel_join_date&quot;, &quot;channel_views&quot;, &quot;channel_description&quot;, &quot;post_date&quot;, ] ).reset_index(drop=True) # merge dfs merged = channel_details.merge(video_details, on=&quot;channel_name&quot;) # drop 2nd url column and rename remaining url col merged.drop(columns=(&quot;url_x&quot;), inplace=True) merged.rename(columns={&quot;url_y&quot;: &quot;url&quot;}, inplace=True) # dtypes to float for views and subscribers merged.subscribers = ( merged.subscribers.str.replace(&quot;M subscribers&quot;, &quot;&quot;).astype(&quot;float&quot;) * 1000000 ) # modify views col dtype to float def fix_views(col): if &quot;M&quot; in col: return float(col.replace(&quot;M views&quot;, &quot;&quot;)) * 1000000 elif &quot;K&quot; in col: return float(col.replace(&quot;K views&quot;, &quot;&quot;)) * 1000 elif &quot;1 year ago&quot; in col: return 0 merged[&quot;views&quot;] = merged[&quot;views&quot;].apply(fix_views) # Correct channel view column to display num only merged[&quot;channel_views&quot;] = ( merged[&quot;channel_views&quot;].str.replace(&quot;,&quot;, &quot;&quot;).str.replace(&quot; views&quot;, &quot;&quot;).astype(&quot;int&quot;) ) # import Videos csv data df_videos = pd.read_csv( &quot;yt_videos_scrap_big_data.csv&quot;, parse_dates=[&quot;Publish Date&quot;, &quot;Upload_date&quot;] ) df_videos.drop( columns=[&quot;Unnamed: 0&quot;, &quot;Duration&quot;, &quot;Channel Name&quot;, &quot;Title&quot;], inplace=True ) # comments dytpe to int df_videos[&quot;Comments&quot;] = ( df_videos[&quot;Comments&quot;].str.replace(&quot;Comments&quot;, &quot;&quot;).str.replace(&quot;,&quot;, &quot;&quot;).astype(&quot;int&quot;) ) # modify likes col dtype to float def fix_likes(col): if &quot;M&quot; in col: return float(col.replace(&quot;M&quot;, &quot;&quot;)) * 1000000 elif &quot;K&quot; in col: return float(col.replace(&quot;K&quot;, &quot;&quot;)) * 1000 else: return float(col) # Fix Likes Column df_videos[&quot;Likes&quot;] = df_videos[&quot;Likes&quot;].apply(fix_likes) # Fix Width and Height, remove &#39;.&#39; and &#39;0&#39; from end of str df_videos[&quot;Width&quot;] = df_videos[&quot;Width&quot;].astype(&quot;str&quot;).str.split(&quot;.&quot;, expand=True)[0] df_videos[&quot;Height&quot;] = df_videos[&quot;Height&quot;].astype(&quot;str&quot;).str.split(&quot;.&quot;, expand=True)[0] vc_merged = merged.merge(df_videos, on=&quot;url&quot;) # rename columns to increase readability in analysis plots and tables vc_merged.rename( columns={ &quot;channel_name&quot;: &quot;Channel Name&quot;, &quot;channel_join_date&quot;: &quot;Channel Join Date&quot;, &quot;channel_views&quot;: &quot;Channel Views (M)&quot;, &quot;subscribers&quot;: &quot;Subscribers (M)&quot;, &quot;Interaction Count&quot;: &quot;Interactations (M)&quot;, &quot;views&quot;: &quot;Video Views (M)&quot;, &quot;Partial Description&quot;: &quot;Video Desc&quot;, &quot;Publish Date&quot;: &quot;Publish Date&quot;, &quot;Upload_date&quot;: &quot;Upload Date&quot;, &quot;Genre&quot;: &quot;Video Genre&quot;, &quot;Width&quot;: &quot;Width&quot;, &quot;Height&quot;: &quot;Height&quot;, &quot;Comments&quot;: &quot;Video Comments&quot;, &quot;title&quot;: &quot;Video Title&quot;, &quot;url&quot;: &quot;Video URL&quot;, }, inplace=True, ) . . Data Cleaning Complete . Fully cleaned and merged data from Youtubes Channels and all Videos. . . Note: The columns Channel Views (M), Subscribers (M), Video Views (M), and Interactions (M) are in millions. . Example: The iJustine Channel has 6.89 M subscribers. | . Data Analysis . Youtube Channels Ordered by Join Date . Note: Join Date is the date that the Youtube Channel was created. . # List of Video Channels yt_chan_jn = ( df.groupby([&quot;Channel Join Date&quot;, &quot;Channel Name&quot;, &quot;Channel Views (M)&quot;])[ &quot;Subscribers (M)&quot; ] .max() .to_frame() .reset_index() ) # rename columns to increase readability yt_chan_jn.rename( columns={ &quot;Channel Name&quot;: &quot;Channel&quot;, &quot;Channel Join Date&quot;: &quot;Join Date&quot;, &quot;Subscribers (M)&quot;: &quot;Subscribers&quot;, &quot;Channel Views (M)&quot;: &quot;Channel Views&quot;, }, inplace=True, ) yt_chan_jn # # style dateframe to highlight highest values yt_chan_jn.style.format( formatter={&quot;Subscribers&quot;: &quot;{:,} M&quot;, &quot;Channel Views&quot;: &quot;{:,} M&quot;} ).background_gradient( subset=[&quot;Channel Views&quot;, &quot;Subscribers&quot;], cmap=&quot;Wistia&quot; ).hide_index() . . Join Date Channel Channel Views Subscribers . 2006-05-07 | iJustine | 1,288.99 M | 6.89 M | . 2007-06-07 | Jon Rettinger | 574.95 M | 1.59 M | . 2007-08-04 | Austin Evans | 1,118.91 M | 5.07 M | . 2008-03-21 | Marques Brownlee | 2,597.03 M | 14.3 M | . 2008-11-24 | Linus Tech Tips | 4,934.74 M | 13.7 M | . 2010-03-24 | Jonathan Morrison | 430.64 M | 2.64 M | . 2010-12-21 | Unbox Therapy | 4,091.68 M | 18.0 M | . 2011-04-03 | Android Authority | 767.86 M | 3.36 M | . 2011-04-20 | Mrwhosetheboss | 1,208.15 M | 7.71 M | . 2012-01-01 | UrAvgConsumer | 430.38 M | 3.11 M | . Top 10 Most Viewed Videos . Note: 70% of the videos in this list are about phones. . # Top 10 Videos by Views top_vwd_chan = ( df.groupby([&quot;Video Title&quot;, &quot;Channel Name&quot;, &quot;Publish Date&quot;])[&quot;Video Views (M)&quot;] .max() .sort_values(ascending=False) .head(10) .reset_index() ) # rename columns to increase readability top_vwd_chan.rename( columns={&quot;Channel Name&quot;: &quot;Channel&quot;, &quot;Video Views (M)&quot;: &quot;Video Views&quot;}, inplace=True ) top_vwd_chan.style.format( formatter={&quot;Video Views&quot;: &quot;{:,} M&quot;, &quot;Publish Date&quot;: &quot;{:%Y-%m-%d}&quot;} ).background_gradient( subset=[&quot;Video Views&quot;, &quot;Publish Date&quot;], cmap=&quot;Wistia&quot; ).hide_index() . . Video Title Channel Publish Date Video Views . iPhone 6 Plus Bend Test | Unbox Therapy | 2014-09-23 | 73.0 M | . Retro Tech: Game Boy | Marques Brownlee | 2019-04-19 | 28.0 M | . BROKE vs PRO Gaming | Austin Evans | 2019-08-03 | 22.0 M | . Samsung Galaxy Fold Unboxing: Magnets! | Marques Brownlee | 2019-04-16 | 22.0 M | . Turn your Smartphone into a 3D Hologram | 4K | Mrwhosetheboss | 2015-08-01 | 22.0 M | . OnePlus 6 Review: Right On the Money! | Marques Brownlee | 2018-05-25 | 21.0 M | . This Smartphone Changes Everything... | Unbox Therapy | 2018-06-19 | 21.0 M | . The 4 Dollar Android Smartphone | Unbox Therapy | 2016-03-11 | 20.0 M | . This Cup Is Unspillable - What Magic Is This? | Unbox Therapy | 2016-07-03 | 20.0 M | . Unboxing The $20,000 Smartphone | Unbox Therapy | 2016-12-25 | 19.0 M | . Youtube Channels Grouped by Total Video Views . Sum of all videos views for each channel. . Note: There is an obvious relationship between Subscribers and Video View counts. . # Total Views by Channel chan_views = ( df.groupby([&quot;Channel Name&quot;, &quot;Subscribers (M)&quot;])[&quot;Video Views (M)&quot;] .sum() .sort_values(ascending=False) .reset_index() ) # rename columns to increase readability chan_views.rename( columns={ &quot;Channel Name&quot;: &quot;Channel&quot;, &quot;Video Views (M)&quot;: &quot;Video Views&quot;, &quot;Subscribers (M)&quot;: &quot;Subscribers&quot;, }, inplace=True, ) chan_views.style.format( formatter={ &quot;Video Views&quot;: &quot;{:,}&quot;, &quot;Video Views&quot;: &quot;{0:,.0f} M&quot;, &quot;Subscribers&quot;: &quot;{:,} M&quot;, } ).background_gradient(subset=[&quot;Video Views&quot;, &quot;Subscribers&quot;], cmap=&quot;Wistia&quot;).hide_index() . . Channel Subscribers Video Views . Unbox Therapy | 18.0 M | 1,522 M | . Marques Brownlee | 14.3 M | 1,286 M | . Linus Tech Tips | 13.7 M | 1,158 M | . Mrwhosetheboss | 7.71 M | 816 M | . Austin Evans | 5.07 M | 600 M | . iJustine | 6.89 M | 597 M | . Android Authority | 3.36 M | 288 M | . Jonathan Morrison | 2.64 M | 249 M | . UrAvgConsumer | 3.11 M | 249 M | . Jon Rettinger | 1.59 M | 193 M | . Top 10 Liked Videos . Note: The following top 10 liked videos don&#39;t review a tech product. . &quot;Reflecting on the Color of My Skin&quot; created by Marques Brownlee | &quot;I&#39;ve been thinking of retiring&quot; created by Linus Tech Tips | . # Top 10 Videos by Views top_lkd_chan = ( df.groupby([&quot;Video Title&quot;, &quot;Channel Name&quot;, &quot;Publish Date&quot;])[&quot;Likes&quot;] .max() .sort_values(ascending=False) .head(10) .reset_index() ) # rename columns to increase readability top_lkd_chan.rename(columns={&quot;Channel Name&quot;: &quot;Channel&quot;}, inplace=True) top_lkd_chan.style.format( formatter={&quot;Likes&quot;: &quot;{:,}&quot;, &quot;Publish Date&quot;: &quot;{:%Y-%m-%d}&quot;} ).background_gradient(subset=[&quot;Likes&quot;, &quot;Publish Date&quot;], cmap=&quot;Wistia&quot;).hide_index() . . Video Title Channel Publish Date Likes . How THIS wallpaper kills your phone. | Mrwhosetheboss | 2020-06-04 | 831,000.0 | . Reflecting on the Color of My Skin | Marques Brownlee | 2020-06-04 | 620,000.0 | . PlayStation 5 Unboxing &amp; Accessories! | Marques Brownlee | 2020-10-27 | 572,000.0 | . Talking Tech with Elon Musk! | Marques Brownlee | 2018-08-17 | 497,000.0 | . I&#39;ve been thinking of retiring. | Linus Tech Tips | 2020-01-22 | 480,000.0 | . How THIS instagram story kills your phone. | Mrwhosetheboss | 2021-05-06 | 456,000.0 | . iPhone 12 Unboxing Experience + MagSafe Demo! | Marques Brownlee | 2020-10-20 | 426,000.0 | . This Cup Is Unspillable - What Magic Is This? | Unbox Therapy | 2016-07-03 | 415,000.0 | . AirPods Max Unboxing &amp; Impressions: $550?! | Marques Brownlee | 2020-12-10 | 396,000.0 | . RETRO TECH: MACINTOSH | Marques Brownlee | 2020-12-10 | 396,000.0 | . Videos Likes per Video (Scatter Plot) . # Top Video Likes Over Time (Scatter Plot) import plotly.express as px import plotly.graph_objects as go # set global plot colors # plotly marker colors mcolors = &quot;#1f77b4&quot; # light blue # wordcloud letters cmaps = &quot;Wistia&quot; cmaps_r = &quot;Wistia_r&quot; # plotly backround wtbckgnd = {&quot;plot_bgcolor&quot;: &quot;rgba(255,255,255, 0.9)&quot;} # white background blkbackground = {&quot;plot_bgcolor&quot;: &quot;rgba(0, 0, 0, 0.5)&quot;} # black background fig = px.scatter( df, y=&quot;Likes&quot;, x=&quot;Publish Date&quot;, color=&quot;Likes&quot;, hover_name=&quot;Video Title&quot;, hover_data=[&quot;Channel Name&quot;], color_continuous_scale=&quot;solar_r&quot;, ) fig.update_layout( wtbckgnd, # set background to white title={ &quot;text&quot;: &quot;Top Video Likes Over Time&quot;, &quot;y&quot;: 0.88, &quot;x&quot;: 0.5, &quot;xanchor&quot;: &quot;center&quot;, &quot;yanchor&quot;: &quot;top&quot;, }, xaxis_title=&quot;Video Publish Date&quot;, yaxis_title=&quot;No. of Likes&quot;, ) fig.show() . . . . Word Frequency in Video Titles (Bar Plot) . Out of 2,000 videos titles . import texthero as hero from texthero import preprocessing from texthero import stopwords # create a custom cleaning pipeline custom_pipeline = [ preprocessing.fillna # , preprocessing.lowercase , preprocessing.remove_digits, preprocessing.remove_punctuation, preprocessing.remove_diacritics # , preprocessing.remove_stopwords , preprocessing.remove_whitespace, ] # , preprocessing.stem] default_stopwords = stopwords.DEFAULT # add a list of stopwords to the stopwords custom_stopwords = default_stopwords.union(set([&quot;The&quot;, &quot;vs&quot;])) # pass the custom_pipeline to the pipeline argument df[&quot;clean_title&quot;] = hero.clean(df[&quot;Video Title&quot;], pipeline=custom_pipeline) # Call remove_stopwords and pass the custom_stopwords list df[&quot;clean_title&quot;] = hero.remove_stopwords(df[&quot;clean_title&quot;], custom_stopwords) tw = hero.visualization.top_words(df[&quot;clean_title&quot;]).head(10).to_frame() tw.reset_index(inplace=True) tw.rename(columns={&quot;index&quot;: &quot;word&quot;, &quot;clean_title&quot;: &quot;freq&quot;}, inplace=True) fig = go.Figure([go.Bar(x=tw.word, y=tw.freq, textposition=&quot;auto&quot;)]) fig.update_layout( wtbckgnd, # set background to white title={ &quot;text&quot;: &quot;Word Frequency in Video Titles&quot;, &quot;y&quot;: 0.88, &quot;x&quot;: 0.5, &quot;xanchor&quot;: &quot;center&quot;, &quot;yanchor&quot;: &quot;top&quot;, }, yaxis=dict(title=&quot;Word Count&quot;), ) fig.update_traces(marker_color=&quot;orange&quot;) . . . . Word Frequency in Video Titles (Word Cloud) . import texthero as herofrom # Word cloud of top words from clean_title herofrom.wordcloud( df.clean_title, max_words=200, # contour_color=&#39;red&#39;, background_color=&quot;white&quot;, colormap=&quot;Oranges&quot;, height=500, width=800, ) . . Using Kmeans to Cluster of Video Titles . Grouping text content of video titles according to their similarities. . # Add pca value to dataframe to use as visualization coordinates df[&quot;pca&quot;] = df[&quot;clean_title&quot;].pipe(hero.tfidf).pipe(hero.pca) # Add k-means cluster to dataframe df[&quot;kmeans&quot;] = df[&quot;clean_title&quot;].pipe(hero.tfidf).pipe(hero.kmeans) hero.scatterplot(df, &quot;pca&quot;, color=&quot;kmeans&quot;, hover_data=[&quot;Video Title&quot;]) . . . . Correlations . df.drop(columns=[&quot;Unnamed: 0&quot;,&quot;Width&quot;,&quot;Height&quot;]).corr().style.background_gradient( subset=[ &quot;Channel Views (M)&quot;, &quot;Subscribers (M)&quot;, &quot;Video Views (M)&quot;, &quot;Likes&quot;, &quot;Video Comments&quot;, &quot;Interactations (M)&quot;, ], cmap=&quot;Wistia&quot;, ) . . Channel Views (M) Subscribers (M) Video Views (M) Likes Video Comments Interactations (M) . Channel Views (M) 1.000000 | 0.907635 | 0.586217 | 0.570409 | 0.138889 | 0.583878 | . Subscribers (M) 0.907635 | 1.000000 | 0.659920 | 0.652701 | 0.163038 | 0.659026 | . Video Views (M) 0.586217 | 0.659920 | 1.000000 | 0.708341 | 0.155869 | 0.996397 | . Likes 0.570409 | 0.652701 | 0.708341 | 1.000000 | 0.235680 | 0.715335 | . Video Comments 0.138889 | 0.163038 | 0.155869 | 0.235680 | 1.000000 | 0.156037 | . Interactations (M) 0.583878 | 0.659026 | 0.996397 | 0.715335 | 0.156037 | 1.000000 | . Conclusion . Video Comment numbers have very little correlation to any data that was obtained in this project. | . Resources . Top 25 Selenium Functions That Will Make You Pro In Web Scraping . | How to build a Web Scraper or Bot in Python using Selenium . | Web Scraping: Introduction, Best Practices &amp; Caveats . | Web Scraping Job Postings from Indeed.com using Selenium . | . How I Use Selenium to Automate the Web With Python. Pt1 - John Watson Rooney | .",
            "url": "https://drusho.github.io/blog/blog/selenium/web%20scrapping/pandas/youtube/python/2021/07/20/webscrapping-youtube-blog.html",
            "relUrl": "/selenium/web%20scrapping/pandas/youtube/python/2021/07/20/webscrapping-youtube-blog.html",
            "date": " • Jul 20, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Positive COVID-19 Reports in Florida Schools for 2021",
            "content": "Notebook Created by: David Rusho (Github Blog | Tableau | Linkedin) . . Tableau Dashboard Preview: FL School&#39;s Covid-19 Cases . . Introduction . About the Data . . This Data comes from the Florida Department of Health. They release several reports throughout the years that list the number of positive covid-19 cases by school. The types of schools include Elementary, Middle, High School, Charter/Prepatory Schools, and University/Colleges. The data is presented on a pdf, is in wide format, and the school types are note labeled. . Also, the version of Tableau that is being used is Tableau Public. This means that the data that is exported will need to be in an excel format. . Data Cleaning . Import Data . import pandas as pd . . #import pdf and filter cols/rows df = pd.read_csv(&#39;schools_latest.csv&#39;,usecols=[0,1,12,13,14,15,16,17,18,19],skiprows=5) df.sample(2) . . School County Unnamed: 12 Students.1 Teachers.1 Staff.1 Unknown.2 Yes.1 No.1 Unknown.3 . 4230 NaN | NaN | NaN | NaN | Cumulative (Sep 6 - May 22) | NaN | NaN | NaN | NaN | NaN | . 6209 THE WEBSTER SCHOOL | St. Johns | 25 | 14 | 2 | 5 | 4 | 17 | 8 | 0 | . Rename Columns . #rename columns df.columns=[&#39;School&#39;,&#39;County&#39;,&#39;Total_Cases&#39;,&#39;Students&#39;,&#39;Teachers&#39;,&#39;Staff&#39;,&#39;Unknown&#39;,&#39;Symptoms_Yes&#39;,&#39;Symptoms_No&#39;,&#39;Symptoms_Unknown&#39;] df.sample(2) . . School County Total_Cases Students Teachers Staff Unknown Symptoms_Yes Symptoms_No Symptoms_Unknown . 2900 BOYETTE SPRINGS ELEMENTARY SCHOOL | Hillsborough | 24 | 15 | 3 | 4 | 2 | 15 | 3 | 6 | . 3641 SIX MILE CHARTER ACADEMY | Lee | 17 | 15 | 1 | 0 | 1 | 7 | 7 | 3 | . Remove nan values from &#39;School&#39; col . Remove nan values from &#39;County&#39; col . # new df with nan values removed df2 = df1[df1[&#39;County&#39;].isna()==False] df2.sample(3) . School County Total_Cases Students Teachers Staff Unknown Symptoms_Yes Symptoms_No Symptoms_Unknown . 995 COCONUT GROVE MONTESSORI SCHOOL | Dade | 4 | 2 | 0 | 0 | 2 | 1 | 2 | 1 | . 3805 MIAMI NORLAND SENIOR HIGH SCHOOL | Dade | 27 | 16 | 2 | 2 | 7 | 19 | 8 | 0 | . 2619 HENRY S. WEST LABORATORY SCHOOL | Dade | 6 | 5 | 0 | 0 | 1 | 3 | 2 | 1 | . . df2.isna().sum() . . School 0 County 0 Total_Cases 135 Students 0 Teachers 0 Staff 0 Unknown 0 Symptoms_Yes 0 Symptoms_No 0 Symptoms_Unknown 0 dtype: int64 . Remove nan values from &#39;Total Cases&#39; col . df3 = df2[df2.Total_Cases.isna()==False] df3.sample(3) . School County Total_Cases Students Teachers Staff Unknown Symptoms_Yes Symptoms_No Symptoms_Unknown . 4686 PORT CHARLOTTE MIDDLE SCHOOL | Charlotte | 34 | 28 | 2 | 0 | 4 | 25 | 9 | 0 | . 5681 SURGE CHRISTIAN ACADEMY | Pinellas | 4 | 4 | 0 | 0 | 0 | 3 | 1 | 0 | . 6204 VALRICO LAKE ADVANTAGE ACADEMY | Hillsborough | 15 | 10 | 3 | 0 | 2 | 7 | 0 | 8 | . Dataframe contains no nan values. . #df is balanced with nan values removed df3.info() . . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; Int64Index: 6064 entries, 0 to 6603 Data columns (total 10 columns): # Column Non-Null Count Dtype -- -- 0 School 6064 non-null object 1 County 6064 non-null object 2 Total_Cases 6064 non-null object 3 Students 6064 non-null object 4 Teachers 6064 non-null object 5 Staff 6064 non-null object 6 Unknown 6064 non-null object 7 Symptoms_Yes 6064 non-null object 8 Symptoms_No 6064 non-null object 9 Symptoms_Unknown 6064 non-null object dtypes: object(10) memory usage: 521.1+ KB . Drop unnecessary cols . # No way to identify which goups symptoms belong to df4 = df3.drop(columns=[&#39;Symptoms_Yes&#39;,&#39;Symptoms_No&#39;,&#39;Symptoms_Unknown&#39;]).reset_index(drop=True) df4.head(3) . School County Total_Cases Students Teachers Staff Unknown . 0 IRVING &amp; BEATRICE PESKOE K-8 CENTER | Dade | 1 | 0 | 0 | 0 | 1 | . 1 2010 E HILLSBOROUGH AVE, | Hillsborough | 1 | 1 | 0 | 0 | 0 | . 2 A BLESSED ACADEMY | Polk | 1 | 1 | 0 | 0 | 0 | . Concat &#39;School&#39; and &#39;County&#39; cols . Several schools have the same name. Combining School and County columns creates a unique identifier that can be split in Tableau to contain more useful data visualizations. . df4[&#39;School_County&#39;] = (df4.School.str.cat(df4.County, sep=&quot; (&quot;) + &quot; County)&quot;).str.title() df4.head() . School County Total_Cases Students Teachers Staff Unknown School_County . 0 IRVING &amp; BEATRICE PESKOE K-8 CENTER | Dade | 1 | 0 | 0 | 0 | 1 | Irving &amp; Beatrice Peskoe K-8 Center (Dade Cou... | . 1 2010 E HILLSBOROUGH AVE, | Hillsborough | 1 | 1 | 0 | 0 | 0 | 2010 E Hillsborough Ave, (Hillsborough County) | . 2 A BLESSED ACADEMY | Polk | 1 | 1 | 0 | 0 | 0 | A Blessed Academy (Polk County) | . 3 A CHILD&#39;S PLACE MONTESSORI SCHOOL | Duval | 2 | 2 | 0 | 0 | 0 | A Child&#39;S Place Montessori School (Duval County) | . 4 A E F SCHOOLS ALTERNATIVE EDU. FOUNDATION | Broward | 3 | 0 | 1 | 2 | 0 | A E F Schools Alternative Edu. Foundation (Bro... | . Convert df from wide to long using melt() . df_long = df4.melt(id_vars=[&#39;School&#39;,&#39;County&#39;,&#39;School_County&#39;,&#39;Total_Cases&#39;], var_name=&#39;Individual&#39;, value_name=&#39;pos_covid&#39;) df_long.sample(3) . School County School_County Total_Cases Individual pos_covid . 6492 BENNETT ELEMENTARY SCHOOL | Broward | Bennett Elementary School (Broward County) | 22 | Teachers | 5 | . 9414 MENI NINHO MENI NINHA INC | Dade | Meni Ninho Meni Ninha Inc (Dade County) | 1 | Teachers | 0 | . 2962 LEE ADOLESCENT MOTHERS PROGRAM | Lee | Lee Adolescent Mothers Program (Lee County) | 3 | Students | 3 | . Drop &#39;Total Cases&#39; col . # Drop &#39;_Total Cases_&#39; col df_long2 = df_long.drop(columns=&#39;Total_Cases&#39;) df_long2.head(2) . . School County School_County Individual pos_covid . 0 IRVING &amp; BEATRICE PESKOE K-8 CENTER | Dade | Irving &amp; Beatrice Peskoe K-8 Center (Dade Cou... | Students | 0 | . 1 2010 E HILLSBOROUGH AVE, | Hillsborough | 2010 E Hillsborough Ave, (Hillsborough County) | Students | 1 | . Clean &#39;School&#39; names. . Convert to title() case . #Title Case for Schools df_long2.School = df_long2.School.str.title() df_long2.head(2) . School County School_County Individual pos_covid . 0 Irving &amp; Beatrice Peskoe K-8 Center | Dade | Irving &amp; Beatrice Peskoe K-8 Center (Dade Cou... | Students | 0 | . 1 2010 E Hillsborough Ave, | Hillsborough | 2010 E Hillsborough Ave, (Hillsborough County) | Students | 1 | . . Label University/Colleges . #Label University/Colleges df_long2.loc[((df_long2.School.str.contains(&#39;University&#39;)==True) | (df_long2.School.str.contains(&#39;College&#39;)==True)) &amp; (df_long2.School.str.contains(&#39;Middle&#39;)==False) &amp; (df_long2.School.str.contains(&#39;Elementary&#39;)==False) &amp; (df_long2.School.str.contains(&#39;High&#39;)==False) &amp; (df_long2.School.str.contains(&#39;Academy&#39;)==False) &amp; (df_long2.School.str.contains(&#39;Charter&#39;)==False) &amp; (df_long2.School.str.contains(&#39;Prep&#39;)==False) &amp; (df_long2.School.str.contains(&#39;Preparatory&#39;)==False) &amp; (df_long2.School.str.contains(&#39;1st College&#39;)==False) &amp; (df_long2.School.str.contains(&#39;Kids&#39;)==False) &amp; (df_long2.School.str.contains(&#39;Little College&#39;)==False), &#39;School_Type&#39;] = &#39;University/College&#39; df_long2[df_long2[&#39;School&#39;].str.contains(&#39;Uni&#39;)].sample(3) . . School County School_County Individual pos_covid School_Type . 1718 First United Methodist School | Okaloosa | First United Methodist School (Okaloosa County) | Students | 0 | NaN | . 5646 University Of South Florida- St. Petersburg | Pinellas | University Of South Florida- St. Petersburg (P... | Students | 14 | University/College | . 9222 Lynn University | Palm Beach | Lynn University (Palm Beach County) | Teachers | 0 | University/College | . Label High Schools . #Label High Schools df_long2.loc[((df_long2.School.str.contains(&#39;High&#39;)==True) | (df_long2.School.str.contains(&#39;Hs&#39;)==True)) &amp; (df_long2.School.str.contains(&#39;Middle&#39;)==False) &amp; (df_long2.School.str.contains(&#39;Elementary&#39;)==False) &amp; (df_long2.School.str.contains(&#39;Adult&#39;)==False),&#39;School_Type&#39;] = &#39;High School&#39; df_long2[df_long2[&#39;School&#39;].str.contains(&#39;High&#39;)].sample(3) . . School County School_County Individual pos_covid School_Type . 21009 Lake Highland Preparatory School | Orange | Lake Highland Preparatory School (Orange County) | Unknown | 2 | High School | . 15033 Land O&#39; Lakes High Adult Education | Pasco | Land O&#39; Lakes High Adult Education (Pasco County) | Staff | 0 | NaN | . 18987 Central High School | Hernando | Central High School (Hernando County) | Unknown | 1 | High School | . Label Middle Schools . #Identify Middle Schools df_long2.loc[(df_long2.School.str.contains(&#39;High&#39;)==False) &amp; (df_long2.School.str.contains(&#39;Middle&#39;)==True) &amp; (df_long2.School.str.contains(&#39;Elementary&#39;)==False) &amp; (df_long2.School.str.contains(&#39;Adult&#39;)==False),&#39;School_Type&#39;] = &#39;Middle School&#39; df_long2[df_long2[&#39;School&#39;].str.contains(&#39;Middle&#39;)].sample(3) . . School County School_County Individual pos_covid School_Type . 1526 Eisenhower Middle School | Hillsborough | Eisenhower Middle School (Hillsborough County) | Students | 21 | Middle School | . 16696 Ruben Dario Middle School | Dade | Ruben Dario Middle School (Dade County) | Staff | 0 | Middle School | . 16010 Osceola Middle School | Marion | Osceola Middle School (Marion County) | Staff | 6 | Middle School | . Label Elementary . #identify Elementary Schools #some elementary schools have &#39;University&#39; or &#39;College&#39; in their names df_long2.loc[(df_long2.School.str.contains(&#39;High&#39;)==False) &amp; (df_long2.School.str.contains(&#39;Middle&#39;)==False) &amp; (df_long2.School.str.contains(&#39;Elementary&#39;)==True) &amp; (df_long2.School.str.contains(&#39;Adult&#39;)==False),&#39;School_Type&#39;] = &quot;Elementary School&quot; df_long2[df_long2[&#39;School&#39;].str.contains(&#39;Elem&#39;)].sample(3) . . School County School_County Individual pos_covid School_Type . 1591 Estates Elementary School | Collier | Estates Elementary School (Collier County) | Students | 25 | Elementary School | . 3460 Molino Park Elementary | Escambia | Molino Park Elementary (Escambia County) | Students | 4 | Elementary School | . 20557 Howard Drive Elementary School | Dade | Howard Drive Elementary School (Dade County) | Unknown | 2 | Elementary School | . Label unknown schools a &#39;Other&#39; . df_long2.loc[df_long2.School_Type.isna(),&#39;School_Type&#39;] = &#39;No Label&#39; df_long2[df_long2[&#39;School&#39;].str.contains(&#39;Hs&#39;)] . . School County School_County Individual pos_covid School_Type . 565 Brandon Hs | Hillsborough | Brandon Hs (Hillsborough County) | Students | 1 | High School | . 2629 Jp Taravella Hs | Broward | Jp Taravella Hs (Broward County) | Students | 0 | High School | . 6629 Brandon Hs | Hillsborough | Brandon Hs (Hillsborough County) | Teachers | 0 | High School | . 8693 Jp Taravella Hs | Broward | Jp Taravella Hs (Broward County) | Teachers | 0 | High School | . 12693 Brandon Hs | Hillsborough | Brandon Hs (Hillsborough County) | Staff | 0 | High School | . 14757 Jp Taravella Hs | Broward | Jp Taravella Hs (Broward County) | Staff | 0 | High School | . 18757 Brandon Hs | Hillsborough | Brandon Hs (Hillsborough County) | Unknown | 0 | High School | . 20821 Jp Taravella Hs | Broward | Jp Taravella Hs (Broward County) | Unknown | 1 | High School | . Change dtype of pos_covid to &#39;int&#39; . #Change dtype of pos_covid to &#39;int&#39; df_long2.pos_covid = df_long2.pos_covid.str.replace(&#39;,&#39;,&#39;&#39;).astype(&#39;int&#39;) df_long2.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 24256 entries, 0 to 24255 Data columns (total 6 columns): # Column Non-Null Count Dtype -- -- 0 School 24256 non-null object 1 County 24256 non-null object 2 School_County 24256 non-null object 3 Individual 24256 non-null object 4 pos_covid 24256 non-null int64 5 School_Type 24256 non-null object dtypes: int64(1), object(5) memory usage: 1.1+ MB . . Data Analysis . Groupby school type . #grouby school type df_long2.groupby(&#39;School_Type&#39;).count() . . School County School_County Individual pos_covid . School_Type . Elementary School 6748 | 6748 | 6748 | 6748 | 6748 | . High School 2536 | 2536 | 2536 | 2536 | 2536 | . Middle School 2236 | 2236 | 2236 | 2236 | 2236 | . No Label 11564 | 11564 | 11564 | 11564 | 11564 | . University/College 1172 | 1172 | 1172 | 1172 | 1172 | . Groupby School Type and Individuals . df_long2.groupby([&#39;School_Type&#39;,&#39;Individual&#39;])[&#39;pos_covid&#39;].sum().to_frame().reset_index(). . . School_Type Individual pos_covid . 0 Elementary School | Staff | 1978 | . 1 Elementary School | Students | 23831 | . 2 Elementary School | Teachers | 3300 | . 3 Elementary School | Unknown | 3763 | . 4 High School | Staff | 1161 | . 5 High School | Students | 29353 | . 6 High School | Teachers | 1491 | . 7 High School | Unknown | 1962 | . 8 Middle School | Staff | 720 | . 9 Middle School | Students | 13081 | . 10 Middle School | Teachers | 1057 | . 11 Middle School | Unknown | 1248 | . 12 No Label | Staff | 1680 | . 13 No Label | Students | 25826 | . 14 No Label | Teachers | 2515 | . 15 No Label | Unknown | 3307 | . 16 University/College | Staff | 1308 | . 17 University/College | Students | 19201 | . 18 University/College | Teachers | 286 | . 19 University/College | Unknown | 1313 | . Groupby County and Positive Covid Cases . # groupby county and postive covid cases df_long2.groupby(&#39;County&#39;)[&#39;pos_covid&#39;].sum().to_frame().reset_index().sort_values(by=&#39;pos_covid&#39;,ascending=False).head(15) . . County pos_covid . 12 Dade | 16672 | . 5 Broward | 10523 | . 28 Hillsborough | 10268 | . 49 Palm Beach | 8801 | . 47 Orange | 8610 | . 36 Leon | 6139 | . 35 Lee | 5588 | . 52 Polk | 5524 | . 15 Duval | 4996 | . 51 Pinellas | 4712 | . 50 Pasco | 4541 | . 0 Alachua | 4504 | . 63 Volusia | 3450 | . 4 Brevard | 3137 | . 56 Seminole | 2675 | . Groupby Select County and Individuals . County Selected = &#39;Orange&#39; county . #collapse # groupby one county and individuals var = &#39;Orange&#39; sel_county = df_long2[df_long2[&#39;County&#39;]==var] sel_county.groupby([&#39;County&#39;,&#39;Individual&#39;])[&#39;pos_covid&#39;].sum().to_frame().reset_index() . . County Individual pos_covid . 0 Orange | Staff | 239 | . 1 Orange | Students | 6976 | . 2 Orange | Teachers | 229 | . 3 Orange | Unknown | 1166 | . Export Data to Excel Format . df_long2 = df_long2.sort_values(by=[&#39;County&#39;,&#39;School&#39;]).reset_index(drop=True) df_long2.to_excel(&#39;Florida_Schools_COVID19_Cases_2020_2021.xls&#39;) .",
            "url": "https://drusho.github.io/blog/blog/pandas/python/data%20cleaning/tableau/covid19/2021/07/09/fl-schools-covid19-2021.html",
            "relUrl": "/pandas/python/data%20cleaning/tableau/covid19/2021/07/09/fl-schools-covid19-2021.html",
            "date": " • Jul 9, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Data Analysis of Reddit's /r/Politics",
            "content": "Notebook Created by: David Rusho (Github Blog | Tableau | Linkedin) . Introduction . About the Data . What is Reddit? . . Reddit is an American social news aggregation, web content rating, and discussion website. Registered members submit content to the site such as links, text posts, images, and videos, which are then voted up or down by other members. . . Subreddits . . Posts are organized by subject into user-created boards called &quot;communities&quot; or &quot;subreddits&quot;, which cover a variety of topics such as news, politics, religion, science, movies, video games, music, books, sports, fitness, cooking, pets, and image-sharing. . . Upvotes/Downvotes . . Submissions with more up-votes appear towards the top of their subreddit and, if they receive enough up-votes, ultimately on the site&#39;s front page . . Subreddit Tabs . . At the top of each page on Reddit, you will see a selection of tabs marked Hot, New, Rising, Controversial, Top, Gilded, and Wiki. . Hot posts are the posts that have been getting the most upvotes and comments recently on that subreddit. This is the tab that will be used for this notebook. . Project Goals . This notebook will focus on &#39;Hot&#39; subreddit tab posts due to their focus on upvotes and recent comments. Data from /r/politics will be scrapped using python library Praw. Analysis will include determining top posts for this subreddit and understanding what factors contributed to their ranking beyond most upvotes and comments. Such as the correlation between comments and points, word frequency and semantic analysis of post titles . Summary of Results . Correlation of Post Score and Number of Comments . . A heatmap that was ran through Seaborn showed there was a very positive correlation between the number of comments and the score of a posts (0.89). . . Word Frequency of Post Titles . . Word frequency showed that Biden and Trump were the most popular key words, followed by GOP. . . Sentiment Analysis . . The majority of posts in /r/politics were found be neutral, followed by negative. . Data Collection and Cleaning . Import Libraries . !pip install praw !pip install vaderSentiment !pip install texthero . . from configparser import ConfigParser import datetime as dt import matplotlib.pyplot as plt import numpy as np import pandas as pd import plotly.express as px import plotly.graph_objects as go import praw import seaborn as sns import texthero as herofrom from texthero import preprocessing from texthero import stopwords from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer import warnings warnings.filterwarnings(&#39;ignore&#39;) . . Praw (Reddit API) Setup . # praw setup reddit = praw.Reddit(client_id = cid, #peronal use script client_secret = csec, #secret token usernme = username, #profile username password = password, #profile password user_agent = ua, #user agent check_for_async=False) . . Organize and Clean Data . Scrap 500 Reddit Posts from /r/poltics from &#39;Hot&#39; tab. . # list for df conversion posts = [] # select a subreddit to scrape sub = &#39;politics&#39; # return 500 new posts new_bets = reddit.subreddit(sub).hot(limit=500) # return selected reddit post attributes for post in new_bets: posts.append([post.title, post.selftext, post.score, post.upvote_ratio, post.num_comments, post.created_utc, post.is_original_content, post.url]) # create df, rename columns, and make dtype for all data a str posts = pd.DataFrame(posts, columns=[&#39;title&#39;, &#39;post&#39;, &#39;score&#39;, &#39;upvote_ratio&#39;, &#39;comments&#39;, &#39;created&#39;, &#39;original_content&#39;, &#39;url&#39;], dtype=&#39;str&#39;) posts.sample(3) . . title post score upvote_ratio comments created original_content url . 428 A judge blocked Florida Gov. Ron DeSantis&#39; &#39;de... | | 1563 | 0.98 | 107 | 1625166787.0 | False | https://www.businessinsider.com/florida-ron-de... | . 483 Garland orders halt to any further federal exe... | | 147 | 0.92 | 1 | 1625182268.0 | False | https://abcnews.go.com/Politics/garland-orders... | . 218 Biden administration formally launches effort ... | | 3784 | 0.98 | 245 | 1625270143.0 | False | https://www.inquirer.com/news/nation-world/bid... | . Column Descriptions . Heading Description . title | The title of the submission. | . post | The submissions’ selftext - an empty string if a link post. | . score | The number of upvotes for the submission. | . upvote_ratio | The percentage of upvotes from all votes on the submission. | . comments | The number of comments on the submission. | . created | Time the submission was created, represented in Unix Time. | . original_content | Whether or not the submission has been set as original content. | . url | The URL the submission links to, or the permalink if a selfpost. | . Change &#39;created&#39; Column Dtype to datetime . # created timestamp column to represent correct created column data posts[&#39;created&#39;] = pd.to_datetime(posts[&#39;created&#39;], unit=&#39;s&#39;) posts[&#39;created&#39;].head(1) . 0 2021-07-05 16:00:02 Name: created, dtype: datetime64[ns] . . Show Dataframe Dtypes . # change dytpe of score and comments cols to int posts[[&#39;comments&#39;,&#39;score&#39;]] = posts[[&#39;comments&#39;,&#39;score&#39;]].astype(&#39;int&#39;) posts[&#39;upvote_ratio&#39;] = posts[&#39;upvote_ratio&#39;].astype(&#39;float&#39;) . . posts.info() . . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 500 entries, 0 to 499 Data columns (total 8 columns): # Column Non-Null Count Dtype -- -- 0 title 500 non-null object 1 post 500 non-null object 2 score 500 non-null int64 3 upvote_ratio 500 non-null float64 4 comments 500 non-null int64 5 created 500 non-null datetime64[ns] 6 original_content 500 non-null object 7 url 500 non-null object dtypes: datetime64[ns](1), float64(1), int64(2), object(4) memory usage: 31.4+ KB . Clean Post Titles (NLP Preprossing) . #Clean post titles using texthero posts[&#39;clean_title&#39;] = herofrom.clean(posts[&#39;title&#39;]) posts[&#39;clean_title&#39;].sample(3) . . 497 nancy pelosi signals hard line formation janua... 430 foreign media skewer joe biden &#39;barely cogent ... 281 biden administration freezes u assets myanmar ... Name: clean_title, dtype: object . Data Exploration . Top 10 Popular Posts by Score . # Top 10 Popular posts based on score top_posts = posts.groupby([&#39;title&#39;])[&#39;score&#39;,&#39;upvote_ratio&#39;].sum().sort_values(by=&#39;score&#39;,ascending=False).reset_index() top_posts[[&#39;score&#39;,&#39;upvote_ratio&#39;,&#39;title&#39;]].head(3) . . score upvote_ratio title . 0 59394 | 0.82 | Charles Booker makes it official, announces ru... | . 1 56462 | 0.89 | Dominion has subpoenaed Rudy Giuliani, Sidney ... | . 2 51924 | 1.83 | Biden says teachers deserve ‘a raise, not just... | . Word Frequency of Post Titles (Wordcloud) . # Word cloud of top words from clean_title herofrom.wordcloud(posts.clean_title, max_words=200, contour_color=&#39;&#39;, background_color=&#39;white&#39;, colormap=cmaps, height = 500, width=800) . . Top 25 Words From Post Titles (Bar Plot) . # create new dateframe of top words tw = herofrom.visualization.top_words(posts[&#39;clean_title&#39;]).head(20).to_frame() tw.reset_index(inplace=True) tw.rename(columns={&#39;index&#39;:&#39;word&#39;,&#39;clean_title&#39;:&#39;freq&#39;},inplace=True) #remove word less than 2 chars tw2 = tw[tw[&#39;word&#39;].str.len() &gt;=2] tw2 = tw2.sort_values(by=&#39;freq&#39;,ascending=False) tw2.head(3) . . word freq . 0 biden | 85 | . 1 trump | 67 | . 2 gop | 43 | . Word Frequency of Post Titles (Bar Plot) . # Top 25 Words From Post Titles fig = go.Figure([go.Bar(x=tw2.word, y=tw2.freq, textposition=&#39;auto&#39;)]) fig.update_layout(wtbckgnd, #set background to white title={&#39;text&#39;: f&#39;Top 25 Words in /r/politics Post Titles ({today})&#39;, &#39;y&#39;:0.88,&#39;x&#39;:0.5,&#39;xanchor&#39;: &#39;center&#39;,&#39;yanchor&#39;: &#39;top&#39;}, yaxis=dict(title=&#39;Word Count&#39;)) fig.update_traces(marker_color=mcolors) #set market colors to light blue fig.show() . . . . Post Scores vs Comments (Scatter Plot) . # Post Scores vs Comments fig = go.Figure(data=go.Scatter(x=posts.comments, y=posts.score, mode=&#39;markers&#39;, text=posts.title)) # hover text goes here fig.update_layout(wtbckgnd, #set background to white title={&#39;text&#39;: f&quot;/r/politics Posts&#39; Scores vs Comments ({today})&quot;, &#39;y&#39;:0.88,&#39;x&#39;:0.5,&#39;xanchor&#39;: &#39;center&#39;,&#39;yanchor&#39;: &#39;top&#39;}, xaxis_title=&quot;Post Score&quot;, yaxis_title=&quot;No. of Comments&quot;,) fig.update_traces(marker_color=mcolors) #set market colors to light blue fig.show() . . . . Post Scores by Post Counts (Histrogram Plot) . fig = px.histogram(posts, x=&quot;score&quot;) fig.update_layout(wtbckgnd, #set background to white title={&#39;text&#39;: f&#39;Post Scores by Post Counts&#39;, &#39;y&#39;:0.88,&#39;x&#39;:0.5,&#39;xanchor&#39;: &#39;center&#39;,&#39;yanchor&#39;: &#39;top&#39;}, yaxis=dict(title=&#39;Post Count&#39;), xaxis=dict(title=&#39;Post Score&#39;)) fig.update_traces(marker_color=mcolors) #set market colors to light blue fig.show() . . . . Sentiment Analysis of Post Titles . Scale for determining sentiment . positive: compound score&gt;=0.05 neutral: compound score between -0.05 and 0.05 negative: compound score&lt;=-0.05 . #Sentiment Analysis of Post Titles analyzer = SentimentIntensityAnalyzer() posts[&#39;neg&#39;] = posts[&#39;title&#39;].apply(lambda x:analyzer.polarity_scores(x)[&#39;neg&#39;]) posts[&#39;neu&#39;] = posts[&#39;title&#39;].apply(lambda x:analyzer.polarity_scores(x)[&#39;neu&#39;]) posts[&#39;pos&#39;] = posts[&#39;title&#39;].apply(lambda x:analyzer.polarity_scores(x)[&#39;pos&#39;]) posts[&#39;compound&#39;] = posts[&#39;title&#39;].apply(lambda x:analyzer.polarity_scores(x)[&#39;compound&#39;]) posts[[&#39;title&#39;,&#39;neg&#39;,&#39;neu&#39;,&#39;pos&#39;,&#39;compound&#39;]].sample(3) . . title neg neu pos compound . 392 Biden struggles to answer Russia question at p... | 0.200 | 0.800 | 0.000 | -0.3612 | . 354 Child tax credit checks will start arriving th... | 0.000 | 0.794 | 0.206 | 0.3818 | . 271 Trump under fire for provocative email to supp... | 0.147 | 0.675 | 0.178 | 0.1280 | . Create Sentiment Column Using Compound Numbers . # sentiment col def sentiment(compscore): if compscore &gt;= 0.05: return &#39;positive&#39; elif -0.05 &lt; compscore &lt; 0.05: return &#39;neutral&#39; elif compscore &lt;=-0.05: return &#39;negative&#39; posts[&#39;sentiment&#39;] = posts.compound.apply(sentiment) posts[[&#39;title&#39;,&#39;neg&#39;,&#39;neu&#39;,&#39;pos&#39;,&#39;compound&#39;,&#39;sentiment&#39;]].sample(3) . . title neg neu pos compound sentiment . 126 Op-Ed: What does it mean to be American? Ask a... | 0.000 | 1.000 | 0.000 | 0.0000 | neutral | . 11 Want Better Policing? Make It Easier To Fire B... | 0.330 | 0.279 | 0.391 | 0.0258 | neutral | . 176 They kept the wheels on democracy as Trump tri... | 0.158 | 0.842 | 0.000 | -0.4939 | negative | . Sentiment of Post Titles (Histogram Plot) . # posts.sentiment.value_counts().to_frame().reset_index() fig = px.histogram(posts, x=&quot;compound&quot;, color=&quot;sentiment&quot;, # color_discrete_sequence= px.colors.sequential.Blues color_discrete_sequence=[&quot;#1f77b4&quot;, &quot;#97C3E1&quot;, &quot;#559ACA&quot;]) fig.update_layout(wtbckgnd, #set background to white title={&#39;text&#39;: f&quot;Sentiment of /r/politics Posts ({today})&quot;, &#39;y&#39;:0.95,&#39;x&#39;:0.5,&#39;xanchor&#39;: &#39;center&#39;,&#39;yanchor&#39;: &#39;top&#39;}, xaxis_title=&quot;Compound Score&quot;, yaxis_title=&quot;No. of Posts&quot;,) # fig.update_traces(marker_color=mcolors) #set market colors to light blue . . . . Post Scores vs Compound Sentiment Score (Scatter Plot) . # Post Scores vs Compound Sentiment Score fig = go.Figure(data=go.Scatter(x=posts.compound, y=posts.score, mode=&#39;markers&#39;, text=posts.title)) # hover text goes here fig.update_layout(wtbckgnd, #set background to white title={&#39;text&#39;: &quot;/r/politics Posts&#39; Scores vs Comments&quot;, &#39;y&#39;:0.88,&#39;x&#39;:0.5,&#39;xanchor&#39;: &#39;center&#39;,&#39;yanchor&#39;: &#39;top&#39;}, xaxis_title=&quot;Compound Sentiment Score&quot;, yaxis_title=&quot;Scores&quot;,) fig.update_traces(marker_color=mcolors) #set market colors to light blue fig.show() . . . . Correlation of Dataframe (Heatmap) . Note *Plotly currently doesn&#39;t have simple solution for using dataframes directly with heatmaps. . # Heatmap of Dataframe mask = np.triu(np.ones_like(posts.corr(), dtype=np.bool))# adjust mask and df mask = mask[1:, :-1] corr = posts.corr().iloc[1:,:-1].copy()# plot heatmap fig, ax = plt.subplots(figsize=(11, 9)) sb.heatmap(corr, mask=mask, annot=True, fmt=&quot;.2f&quot;, cmap=&#39;Blues&#39;, vmin=-1, vmax=1, cbar_kws={&quot;shrink&quot;: .8})# yticks plt.yticks(rotation=0) plt.show() . . Conclusion . Correlation of Post Score and Number of Comments . Heatmap run through Seaborn showed there was a very positive correlation between the number of comments and the score of a posts (0.89). . . Word Frequency of Post Titles . Word frequency showed that presidents Biden and Trump were the most popular key words, followed by &#39;GOP&#39;. . . Sentiment Analysis . The Majority of posts in /r/politics were found be Neutral, followed by negative. . Resources . PRAW: The Python Reddit API Wrapper . | Ultimate Beginners Guide to Collecting Text for Natural Language Processing (NLP) with Python — Twitter, Reddit, Genius and More Collect Text through APIs and Web Scraping . | How to scrape Reddit with Python . | Try TextHero: The Absolute Simplest way to Clean and Analyze Text in Pandas . | How to Use Texthero to Prepare a Text-based Dataset for Your NLP Project . | How to Run Sentiment Analysis in Python using VADER . | How to read and write configuration (.ini) file in python . | Understanding Reddit: A beginner’s guide to the front page of the internet . | Tools Used . Pandas | Plotly | Praw (reddit api tool) 4.Texthero (NLP tool) |",
            "url": "https://drusho.github.io/blog/blog/api/nlp/pandas/plotly/texthero/praw/reddit/python/2021/07/05/reddit-politics-eda.html",
            "relUrl": "/api/nlp/pandas/plotly/texthero/praw/reddit/python/2021/07/05/reddit-politics-eda.html",
            "date": " • Jul 5, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "US Firework Sales and Injuries (Part 2)",
            "content": "Notebook Created by: David Rusho (Github Blog | Tableau | Linkedin) . . Tableau Dashboard: US Fireworks Inuries (2016-2021) . . Introduction . Fireworks in the US are commonly used during the 4th of July and New Years&#39; celebrations. Besides being nice to look at fireworks also cause a lot of apparent problems. . Fires . &quot;More than one-quarter (28 percent) of fires started by fireworks in 2014–2018 were reported on the Fourth of July.&quot; . Personal Injury . &quot;The Consumer Product Safety Commission (CPSC) estimates that 9,100 consumer fireworks-related injuries were seen in US hospital emergency departments in 2018.&quot; . Project Goals . To provide an overview of the types of incidents that involve fireworks. This includes understanding which age groups are most affected and the frequency of injury types. Insights from this analysis may help prevent future injuries or at least assist with increasing awareness of the dangers that fireworks can cause. . In addition to the above analysis, we&#39;ll take a look into sales data for all US states for the last five years (2016-2021). . Summary of Results . Age group of 0-20 showed the highest rate of injury. . . . Injuries to the hands, face, and eyes were the most common, while injuries to lower extremities were less common. . . . Missouri held the record for most spent on fireworks (over 250 million dollars over the past 5 years). . . . There was no significate correlation between the number of injuries in a year compared to the number of sales. . Import Libraries . # install libraries to save plotly images to disk %%capture !pip install kaleido !pip install plotly&gt;=4.0.0 !wget https://github.com/plotly/orca/releases/download/v1.2.1/orca-1.2.1-x86_64.AppImage -O /usr/local/bin/orca !chmod +x /usr/local/bin/orca !apt-get install xvfb libgtk2.0-0 libgconf-2- !pip install texthero . . import matplotlib.pyplot as plt import numpy as np import pandas as pd import plotly.express as px import seaborn as sns import texthero as hero from texthero import preprocessing from texthero import stopwords import warnings warnings.filterwarnings(&#39;ignore&#39;) . . [nltk_data] Downloading package stopwords to /root/nltk_data... [nltk_data] Unzipping corpora/stopwords.zip. . Import Firework Injury Datasets . # Import clean injury dataframe injury = &#39;https://github.com/drusho/fireworks_data_exploration/raw/main/data/data_clean/df_injury_clean.csv&#39; df_injury = pd.read_csv(injury,usecols=[1,2,3,4,5,6,7,8,9,10]) df_injury.head(3) . Treatment_Date Age Sex Alcohol Drug Narrative Incident Locale Body_Part Diagnosis Disposition . 0 1/1/16 | 39 | Male | NaN | NaN | 39YOM WAS LIGHTING BOTTLE ROCKETS AND ONE FLEW... | Home | Eyeball | Contusions, Abrasions | Treated/Untreated and Released | . 1 1/1/16 | 13 | Male | NaN | NaN | 13YOM SOMEONE POINTED FIREWORKS AT HIM FROM 10... | Home | Eyeball | Contusions, Abrasions | Treated/Untreated and Released | . 2 7/5/16 | 31 | Female | NaN | NaN | A 31YOF WAS STRUCK TO EYE WITH PIECE OF FIRECR... | Home | Eyeball | Contusions, Abrasions | Treated/Untreated and Released | . . Time Series Analysis of Injuries (Scatter Plot) . July 4th firework celebrations are the clear winner in number of firework related injuries. . #Time Series Analysis of Injuries (Scatter Plot) #groupby treatment injury_dates = df_injury.groupby(&#39;Treatment_Date&#39;).count().reset_index() injury_dates = injury_dates.rename(columns={&#39;Age&#39;:&#39;Count&#39;}) fig = px.scatter(injury_dates, x=&quot;Treatment_Date&quot;, y=&quot;Count&quot;) fig.update_layout({&quot;plot_bgcolor&quot;:&quot;rgba(255,255,255, 0.9)&quot;}, title={&#39;text&#39;: &quot;Firework Injury Counts by Date&quot;, &#39;y&#39;:.98, &#39;x&#39;:.5, &#39;xanchor&#39;: &#39;center&#39;, &#39;yanchor&#39;: &#39;top&#39;}, xaxis=dict(title=&#39;Date of Injury&#39;), yaxis=dict(title=&#39;Injury Counts&#39;)) fig.update_traces(marker_color=&#39;#1f77b4&#39;) fig.show() . . . . Narrative Column Word Frequency (Bar Plot) . The Narrative column contains a detailed description of each injury reported. . Notice how the body parts &#39;hand&#39;, &#39;eye&#39;, and &#39;face&#39; make the list. . #create a custom cleaning pipeline custom_pipeline = [preprocessing.fillna , preprocessing.lowercase , preprocessing.remove_digits , preprocessing.remove_punctuation , preprocessing.remove_diacritics #, preprocessing.remove_stopwords , preprocessing.remove_whitespace] # , preprocessing.stem] #pass the custom_pipeline to the pipeline argument df_injury[&#39;clean_nar&#39;] = hero.clean(df_injury[&#39;Narrative&#39;], pipeline = custom_pipeline) #add a list of stopwords to the stopwords default_stopwords = stopwords.DEFAULT #Call remove_stopwords and pass the custom_stopwords list custom_stopwords = default_stopwords.union(set([&quot;&#39;&quot;,&quot;I&quot;,&quot;r&quot;,&quot;dx&quot;,&quot;i&quot;,&quot;l&quot;,&quot;yom&quot;,&quot;yow&quot;,&quot;pt&quot;,&quot;type&quot;,&quot;p&quot;,&quot;w&quot;])) df_injury[&#39;clean_nar&#39;] = hero.remove_stopwords(df_injury[&#39;clean_nar&#39;], custom_stopwords) tw = hero.visualization.top_words(df_injury[&#39;clean_nar&#39;]).head(20).reset_index() fig = px.bar(tw, x=&#39;index&#39;, y=&#39;clean_nar&#39;, orientation=&#39;v&#39;) fig.update_layout({&quot;plot_bgcolor&quot;:&quot;rgba(255,255,255, 0.9)&quot;}, title={&#39;text&#39;: &quot;Word Frequency for Injury Reports (2016-2021)&quot;, &#39;y&#39;:.98, &#39;x&#39;:.5, &#39;xanchor&#39;: &#39;center&#39;, &#39;yanchor&#39;: &#39;top&#39;}, xaxis=dict(title=&#39;&#39;), yaxis=dict(title=&#39;Word Counts&#39;)) fig.update_traces(marker_color=&#39;#1f77b4&#39;) fig.show() . . . . Wordcloud of Injury Report (2016-2021) . Words are taken from the Narrative column and are narrowed down to the top 200 words. . Looking at this wordcloud I get the idea that the initial lighting of fireworks is when a lot of injuries occur. . #Wordcloud from Narrative column using hero hero.wordcloud(df_injury[&#39;clean_nar&#39;], max_words=200,contour_color=&#39;&#39;, background_color=&#39;white&#39;,colormap=&#39;Blues&#39;, height = 500,width=800) . . Counting Drug and Alchol Use . The number of people injuried with positive results for drug or alcohol use was very insignificant compared to overall count of injuries. . Drug and Alcohol Usuage (Bar Plot) . #define figure size sns.set(rc={&quot;figure.figsize&quot;:(15, 6)}) #set background to white sns.set_style(&quot;white&quot;) fig, ax = plt.subplots(1,2) sns.countplot(df_injury[&#39;Drug&#39;], ax=ax[0], palette=&quot;Oranges_r&quot;) ax[0].set_title(&#39;Drug Related Incidents&#39;, fontdict = {&#39;fontsize&#39;: 15}) ax[0].set(ylabel=&#39;Counts&#39;, xlabel=&#39;&#39;) sns.countplot(df_injury[&#39;Alcohol&#39;], ax=ax[1], palette=&quot;Blues_r&quot;) ax[1].set_title(&#39;Alcohol Related Incidents&#39;, fontdict = {&#39;fontsize&#39;: 15}) ax[1].set(ylabel=&#39;&#39;, xlabel=&#39;&#39;) # remove spines sns.despine(left=True) #save to png # fig.savefig(&quot;Drug/Alcohol Counts.png&quot;) plt.show() fig.savefig(&#39;Drug_and_Alcohol_Counts.png&#39;) plt.show() . . Incident Counts by Year (Bar Plot) . The most injuries occured in 2020, which also saw its largest increase in firework sales. This correlation doesn&#39;t seem to match prior years. . # Incident Counts by Year BarGraph #define figure size sns.set(rc = {&quot;figure.figsize&quot;:(12,8)}) #set background to white sns.set_style(&quot;white&quot;) treamentDates = df_injury[&#39;Treatment_Date&#39;].dt.year.value_counts().sort_index().reset_index() ax = sns.barplot(y=&quot;Treatment_Date&quot;, x=&quot;index&quot;, data=treamentDates, palette=&quot;Blues&quot;) #set x,y labels ax.set(xlabel=&#39;&#39;, ylabel=&#39;Incident Counts&#39;) #set titles ax.set_title(&#39;Firework Injury Counts by Year&#39;, fontdict = {&#39;fontsize&#39;: 15}) #remove spiens sns.despine(left=True) #save to png ax.figure.savefig(&quot;Firework Injury Counts by Year.png&quot;) plt.show() . . Incident Counts by Gender (Bar Plot) . # Incident Counts by Sex incidentSex = df_injury[&#39;Sex&#39;].value_counts().reset_index(name=&#39;incidents&#39;) #define figure size sns.set(rc={&quot;figure.figsize&quot;:(12, 8)}) #set background to white sns.set_style(&quot;white&quot;) ax = sns.barplot(x=&quot;incidents&quot;, y=&quot;index&quot;, data=incidentSex, palette=&quot;Blues_r&quot;) #set x,y labels ax.set(xlabel=&#39;&#39;, ylabel=&#39;Injury Counts&#39;) #set titles ax.set_title(&#39;Firework Injury Counts by Gender (2016-2020)&#39;, fontdict = {&#39;fontsize&#39;:15}) #remove spines sns.despine(left=True) #save to png ax.figure.savefig(&quot;Firework Injury Counts by Gender.png&quot;) plt.show() . . Incident Counts by Body Part (Bar Plot) . The body parts listed also match the body parts mentioned in word frequency count of narratives. . # Incident Counts by Body Part #define figure size sns.set(rc={&quot;figure.figsize&quot;:(12,8)}) #set background color sns.set_style(&quot;white&quot;) incidentBp = df_injury[&#39;Body_Part&#39;].value_counts().reset_index(name=&#39;incidents&#39;).head(23) ax = sns.barplot(x=&quot;incidents&quot;, y=&quot;index&quot;, data=incidentBp, palette=&quot;Blues_r&quot;) #set x,y labels ax.set(xlabel=&#39;&#39;, ylabel=&#39;&#39;) #set title ax.set_title(&#39;Firework Injury Counts by Body Part (2016-2020)&#39;, fontdict = {&#39;fontsize&#39;:15}) #remove spines sns.despine(left=True) #save to png ax.figure.savefig(&quot;Incident Counts by Body Part.png&quot;) plt.show() . . Age Groups (Histogram) . Details The histogram below shows that the majority of firework related injuries that were reported occured with indivuals under the age of 40. The largest grouping belonged to individuals under the age of 20, which is also below the legal drinking age in the US. # Histogram of Ages #set figsize sns.set(rc={&quot;figure.figsize&quot;:(8, 8)}) #set background color sns.set_style(&quot;white&quot;) ax = sns.histplot(data=df_injury, x=&#39;Age_Fix&#39;, bins=5) #set x,y labels ax.set(xlabel=&#39;Age&#39;, ylabel=&#39;Counts&#39;) #set title ax.set_title(&#39;Firework Injury Counts by Age (2016-2020)&#39;, fontdict = {&#39;fontsize&#39;:15}) #remove spines sns.despine(left=True) #save to png ax.figure.savefig(&quot;Incident Counts by Age_Hist.png&quot;) plt.show() . . Incident Counts by Age (Bar Plot) . # Incident Counts by Age (Age_Fix) #define figure size sns.set(rc={&quot;figure.figsize&quot;:(30, 12)}) #set background color sns.set_style(&quot;white&quot;) incidentAge = df_injury[&#39;Age_Fix&#39;].value_counts().reset_index(name=&#39;incidents&#39;) ax = sns.barplot(y=&quot;incidents&quot;, x=&quot;index&quot;, data=incidentAge, palette=&quot;Blues_r&quot;) #set x,y labels ax.set(xlabel=&#39;Age&#39;, ylabel=&#39;Count&#39;) #set title ax.set_title(&#39;Firework Injury Counts by Age (2016-2020)&#39;, fontdict = {&#39;fontsize&#39;:30}) #remove spines sns.despine(left=True) #save to png ax.figure.savefig(&quot;Incident Counts by Age_Bar.png&quot;) plt.show() . . Firework Incident Counts by Age, Year, and Gender (Swarm Plot) . # Swarm graph by age, year, and gender #define figure size sns.set(rc={&quot;figure.figsize&quot;:(14,10)}) #set background color sns.set_style(&quot;white&quot;) ax = sns.swarmplot(data = df_injury, x = df_injury[&#39;Treatment_Date&#39;].dt.year, y = &quot;Age_Fix&quot;, hue = &quot;Sex&quot;, palette = &quot;Blues_r&quot;) #set x,y labels ax.set(xlabel = &#39;Year&#39;, ylabel = &#39;Age&#39;) #set title ax.set_title(&#39;Firework Injury Counts by Age&#39;, fontdict = {&#39;fontsize&#39;:18}) #remove spines sns.despine(left=True) #save to png ax.figure.savefig(&quot;Incident Counts by Age_Swarm.png&quot;) plt.show() . . Incident Counts by Diagnosis (Bar Plot) . Most injuries were from burns. Combining this information with the analysis above on injuries to body parts (eyes, face, and hands) provides a clear picture of how severe these burn injuries are to people involved. . # Incident Counts by Diagnosis incidentDia = df_injury[&#39;Diagnosis&#39;].value_counts().reset_index(name=&#39;incidents&#39;).head(10) #define figure size sns.set(rc={&quot;figure.figsize&quot;:(14,10)}) #set background color sns.set_style(&quot;white&quot;) ax = sns.barplot(x = &quot;incidents&quot;, y = &quot;index&quot;, data = incidentDia, palette = &quot;Blues_r&quot;) #set x,y labels ax.set(xlabel = &#39;&#39;, ylabel = &#39;&#39;) #set title ax.set_title(&#39;Firework Injury Counts by Diagnosis (2016-2020)&#39;, fontdict = {&#39;fontsize&#39;:18}) #remove spine sns.despine(left=True) # #set y axis labels (shortened longer labels to fit for print out) # ax.set_yticklabels([&#39;Burns&#39;, &#39;Contusions&#39;, &#39;Abrasions&#39;,&#39;Other/Not Stated&#39;, # &#39;Laceration&#39;,&#39;Fracture&#39;,&#39;Amputation&#39;,&#39;Foreign body&#39;, # &#39;Internal organ&#39;,&#39;Strain or Sprain&#39;,&#39;Avulsion&#39;, # &#39;Anoxia&#39;,&#39;Puncture&#39;,&#39;Poisoning&#39;,&#39;Dermatitis&#39;, &#39;Conjunctivitis&#39;, # &#39;Concussions&#39;,&#39;Hematoma&#39;]) #save to png ax.figure.savefig(&quot;Incident Counts by Diagnosis.png&quot;) plt.show() . . Import Firework Sales Data (State) . sales_state = &#39;https://github.com/drusho/fireworks_data_exploration/raw/main/data/data_raw/State%20Imports%20by%20HS%20Commodities.csv&#39; df_sales_st = pd.read_csv(sales_state,skiprows=4,usecols=[0,1,2,3]) df_sales_st.head() . . State Commodity Time Total Value ($US) . 0 Alabama | 360410 Fireworks | 2016 | 29,602,090 | . 1 Alabama | 360410 Fireworks | 2017 | 19,396,430 | . 2 Alabama | 360410 Fireworks | 2018 | 26,399,895 | . 3 Alabama | 360410 Fireworks | 2019 | 28,353,392 | . 4 Alabama | 360410 Fireworks | 2020 | 23,141,950 | . Web Scraping for State Abbreviations . The dataframe was missing state abbreviations that are needed to plot data onto a map using Plotly. Used pandas function &#39;read_hml&#39; to read tables from a website that contained state and state abbreviation data. . #WebScrap State Abbreviations #scrap state names and abbrev states_abrev = pd.read_html(&#39;https://abbreviations.yourdictionary.com/articles/state-abbrev.html&#39;)[0].iloc[1:,:2] #scrap US territory names and abbrev territories = pd.read_html(&#39;https://abbreviations.yourdictionary.com/articles/state-abbrev.html&#39;)[1].iloc[[2,5],:2] #merge dfs st_at = states_abrev.merge(territories,how=&#39;outer&#39;).sort_values(by=0).reset_index(drop=True) #rename cols st_at.rename(columns={0:&#39;State&#39;,1:&#39;Abbrevation&#39;},inplace=True) st_at.head() . . State Abbrevation . 0 Alabama | AL | . 1 Alaska | AK | . 2 Arizona | AZ | . 3 Arkansas | AR | . 4 California | CA | . Merging State Abbreviations with Master Dataframe . #merge abbrevation with state sales data df_sales_st2 = df_sales_st.merge(st_at,how=&#39;inner&#39;) df_sales_st2.head() . . State Commodity Time Total Value ($US) Abbrevation . 0 Alabama | 360410 Fireworks | 2016 | 29,602,090 | AL | . 1 Alabama | 360410 Fireworks | 2017 | 19,396,430 | AL | . 2 Alabama | 360410 Fireworks | 2018 | 26,399,895 | AL | . 3 Alabama | 360410 Fireworks | 2019 | 28,353,392 | AL | . 4 Alabama | 360410 Fireworks | 2020 | 23,141,950 | AL | . Top Sales per State (Bar Plot) . # Visualization State Sales (Bar Plot) st_sales = df_sales_st2.copy() st_sales = st_sales.groupby(&#39;State&#39;)[&#39;Total Value ($US)&#39;].sum().reset_index(name=&#39;Sales&#39;).sort_values(by=&#39;Sales&#39;,ascending=False).reset_index(drop=True).head(20) st_sales.sort_values(by=&#39;Sales&#39;,ascending=True,inplace=True) fig = px.bar(st_sales, y=&#39;State&#39;, x=&#39;Sales&#39;, orientation=&#39;h&#39;, color_continuous_scale=&#39;Blues&#39;, color=&quot;Sales&quot;) fig.update_layout({&quot;plot_bgcolor&quot;:&quot;rgba(255,255,255, 0.9)&quot;}, title={&#39;text&#39;: &quot;Firework Total Sales ($USD) 2016-2020&quot;, &#39;y&#39;:.98, &#39;x&#39;:.5, &#39;xanchor&#39;: &#39;center&#39;, &#39;yanchor&#39;: &#39;top&#39;}) fig.show() # # save fig to image # fig.write_image(&quot;Total Firework Sales ($USD) 2016-2020.png&quot;, width=1980, height=1080) # fig.write_html(&quot;Total Firework Sales ($USD) 2016-2020.html&quot;) . . . . State Sales (Scatter Plot) . Note Plotly currently does not have the ability to position colorscales to horizontal. # Visualization State Sales (Scatter Plot) df_sales_st2.sort_values(by=&#39;State&#39;,ascending=False,inplace=True) fig = px.scatter(df_sales_st2, y=&quot;State&quot;, x=&quot;Time&quot;, color=&quot;Total Value ($US)&quot;, size=&#39;Total Value ($US)&#39;, width=800, height=1100, color_continuous_scale=&#39;Blues&#39;) #change background and legend background to white fig.update_layout({&quot;plot_bgcolor&quot;:&quot;rgba(255,255,255, 0.9)&quot;}, # &quot;paper_bgcolor&quot;: &quot;rgba(255,255,255, 0.9)&quot;}, title={&#39;text&#39;: &quot;Firework Sales ($USD)&quot;, &#39;y&#39;:.98, &#39;x&#39;:.5,&#39;xanchor&#39;:&#39;center&#39;, &#39;yanchor&#39;: &#39;top&#39;}, xaxis=dict(title=&#39;&#39;), yaxis=dict(title=&#39;&#39;)) fig.show() # save fig to image fig.write_image(&quot;Firework Sales ($USD) (scatter_plot).png&quot;, width=800, height=1000) fig.write_html(&quot;Firework Sales ($USD) (scatter_plot).html&quot;) . . . . State Sales (Heatmap Plot of US) . fig = px.choropleth(df_sales_st2, # Input Pandas DataFrame locations=&quot;Abbrevation&quot;, # DataFrame column with locations color=&quot;Total Value ($US)&quot;, # DataFrame column with color values hover_name=&quot;Abbrevation&quot;, # DataFrame column hover info locationmode = &#39;USA-states&#39;, # Set to plot as US States color_continuous_scale=&#39;Blues&#39;) fig.update_layout( title={ &#39;text&#39;: &quot;Firework Total Sales ($USD) 2016-2020&quot;, &#39;y&#39;:.95, &#39;x&#39;:.5, &#39;xanchor&#39;: &#39;center&#39;, &#39;yanchor&#39;: &#39;top&#39;}, geo_scope=&#39;usa&#39;) # Plot only the USA instead of globe fig.show() # save fig to image fig.write_image(&quot;Total State Firework Sales ($USD) 2016-2020 (map).png&quot;, width=1980, height=1080) fig.write_html(&quot;Total State Firework Sales ($USD) 2016-2020 (map).html&quot;) . . . . Comparing Sales and Injury Reports . Total Sales Groupby Year . # Total Sales groupby Year sales_year = df_sales_st2.groupby(df_sales_st2[&#39;Time&#39;].dt.year).sum().reset_index(drop=False) sales_year.rename(columns={&#39;Time&#39;:&#39;Year&#39;,&#39;Total Value ($US)&#39;:&#39;Sales&#39;},inplace=True) sales_year . . Year Sales . 0 2016 | 307825710 | . 1 2017 | 279962808 | . 2 2018 | 331072715 | . 3 2019 | 320021354 | . 4 2020 | 300987616 | . 5 2021 | 157186415 | . Total Injuries Groupby Year . # df_injury.groupby([&#39;Treatment_Date&#39;] df_injury_count = df_injury.groupby(df_injury[&#39;Treatment_Date&#39;].dt.year)[&#39;Age&#39;].count().reset_index(name=&#39;Count&#39;) df_injury_count.rename(columns={&#39;Treatment_Date&#39;:&#39;Year&#39;},inplace=True) df_injury_count . . Year Count . 0 2016 | 268 | . 1 2017 | 329 | . 2 2018 | 234 | . 3 2019 | 261 | . 4 2020 | 440 | . Merging Sales and Injury DataFrame on Year . # Merge sales and injury dfs on year df_merged = sales_year.merge(df_injury_count,how=&#39;left&#39;) df_merged . . Year Sales Count . 0 2016 | 307825710 | 268.0 | . 1 2017 | 279962808 | 329.0 | . 2 2018 | 331072715 | 234.0 | . 3 2019 | 320021354 | 261.0 | . 4 2020 | 300987616 | 440.0 | . 5 2021 | 157186415 | NaN | . Determining Correlation of New DataFrame . # Correlation df_merged.corr() . . Year Sales Count . Year 1.000000 | -0.585906 | 0.529818 | . Sales -0.585906 | 1.000000 | -0.590087 | . Count 0.529818 | -0.590087 | 1.000000 | . Conclusion . Injuries . Age groups of 0-20 showed the highest rate of injury. Injury rates by age decrease with age tend to slowly decrease after age 20. The 60+ age groups showed the lowest rate of injury. . Injuries to the hands, face, and eyes were the most common, while injuries to lower extremities were less common. This is reflected numerous times in the data, such as with word frequency of injury narratives, where an explanation is given for how a person was injured. . Time series analysis showed that the month of July has the highest frequency of firework related injuries. . Sales . Missouri held the record for most spent on fireworks (over 250 million dollars over the past 5 years). For comparison, Alaska spent around 560,000 dollars in the last five years. Sales for fireworks saw a considerable increase in sales during 2020, most likely due to COVID-19. . Correlation Between Sales and Injury Counts . There was no significate correlation between the number of injuries in a year compared to the number of sales. . Ending Remarks . Never hold fireworks while lighting them, and hand, eye, and face protection should be worn at all times when fireworks are nearby fireworks. This is especially true if you are a male below the age of 21. . Tools Used . Matplotlib | Pandas | Plotly | Seaborn | Resources . Consumer Product Safety Commision . | USA Trade Census . | National Fire Protection Association: Fireworks fires and injuries . | Source Details Source 1 consisted multiple excel incident reports involving fireworks over the past 5 years taken from the U.S. Consumer Product Safety Commission (CPSC). The NEISS injury data are gathered from the emergency departments (ED) of 96 hospitals selected as a probability sample of all U.S. hospitals with 24-hour EDs and at least 6 inpatient beds. Each participating NEISS hospital is hand-selected by CPSC because it provides an important representation of all other hospitals of its size and unique characteristics in the U.S. Source 2 conatins Sales and Trade data for each state regarding fireworks. Reports data range from 2016 to April 2021.",
            "url": "https://drusho.github.io/blog/blog/pandas/plotly/seaborn/python/tableau/2021/07/03/firework-part-2-eda.html",
            "relUrl": "/pandas/plotly/seaborn/python/tableau/2021/07/03/firework-part-2-eda.html",
            "date": " • Jul 3, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "US Firework Sales and Injuries (Part 1)",
            "content": "Notebook Created by: David Rusho (Github Blog | Tableau | Linkedin) . . Tableau Dashboard: US Fireworks Inuries (2016-2021) . . Introduction . Goals . Combine and clean firework injury data . About the Data . Source: Firework Injury Reports . The following notebook represents steps that were taken to organize and clean 5 excel files that consist of incident reports involving fireforks over the past 5 years from the CPSC (United States Consumer Product Safety Commision). . The data needed to be converted to csv since reading excel into pandas is extremely slow. CSV did create larger files but the trade off for increased reading speeds was worth it. This was done manually through Excel and then exporting each file as a csv. If there were more files a more &quot;pythonic&quot; method would have been used to save time. . Begin Cleaning Process . Import Libraries . import pandas as pd . . Merge csv Files . filelist = [&#39;NEISS_2016.csv&#39;,&#39;NEISS_2017.csv&#39;,&#39;NEISS_2018.csv&#39;,&#39;NEISS_2019.csv&#39;,&#39;NEISS_2020.csv&#39;] df = pd.concat(map(pd.read_csv, filelist)) df.head(3) . CPSC_Case_Number Treatment_Date Age Sex Race Other_Race Hispanic Body_Part Diagnosis Other_Diagnosis ... Fire_Involvement Alcohol Drug Product_1 Product_2 Product_3 Narrative Stratum PSU Weight . 0 160101845 | 1/1/16 | 92 | 1 | 0 | NaN | NaN | 79 | 57 | NaN | ... | 0 | NaN | NaN | 1645 | 1807 | 0 | 92YOM TRYINGO TO TAKE OFF PANTS AND LOST BALAN... | M | 63 | 103.2251 | . 1 160101847 | 1/1/16 | 90 | 1 | 0 | NaN | NaN | 79 | 57 | NaN | ... | 0 | NaN | NaN | 670 | 0 | 0 | 90YOM FELL GETTING OUT OF A RECLINER CHAIR AND... | M | 63 | 103.2251 | . 2 160101848 | 1/1/16 | 71 | 2 | 0 | NaN | NaN | 79 | 57 | NaN | ... | 0 | NaN | NaN | 1807 | 0 | 0 | 71YOF SLIPPED AND FELL TO HER WET KITCHEN FLOO... | M | 63 | 103.2251 | . 3 rows × 25 columns . . df.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; Int64Index: 1791854 entries, 0 to 309369 Data columns (total 25 columns): # Column Dtype -- 0 CPSC_Case_Number int64 1 Treatment_Date object 2 Age int64 3 Sex int64 4 Race int64 5 Other_Race object 6 Hispanic float64 7 Body_Part int64 8 Diagnosis int64 9 Other_Diagnosis object 10 Body_Part_2 float64 11 Diagnosis_2 float64 12 Other_Diagnosis_2 object 13 Disposition int64 14 Location int64 15 Fire_Involvement int64 16 Alcohol float64 17 Drug float64 18 Product_1 int64 19 Product_2 int64 20 Product_3 int64 21 Narrative object 22 Stratum object 23 PSU int64 24 Weight float64 dtypes: float64(6), int64(13), object(6) memory usage: 355.4+ MB . . Reduce DataFrame Size . Filter results to only diplay firework related incidents . Fireworks product code is &#39;1313&#39; | . df1 = df.query(&#39;(Product_1 == 1313) | (Product_2 == 1313) | (Product_3 == 1313)&#39;) df1.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; Int64Index: 1532 entries, 57 to 308881 Data columns (total 25 columns): # Column Non-Null Count Dtype -- -- 0 CPSC_Case_Number 1532 non-null int64 1 Treatment_Date 1532 non-null object 2 Age 1532 non-null int64 3 Sex 1532 non-null int64 4 Race 1532 non-null int64 5 Other_Race 88 non-null object 6 Hispanic 701 non-null float64 7 Body_Part 1532 non-null int64 8 Diagnosis 1532 non-null int64 9 Other_Diagnosis 146 non-null object 10 Body_Part_2 267 non-null float64 11 Diagnosis_2 267 non-null float64 12 Other_Diagnosis_2 28 non-null object 13 Disposition 1532 non-null int64 14 Location 1532 non-null int64 15 Fire_Involvement 1532 non-null int64 16 Alcohol 701 non-null float64 17 Drug 701 non-null float64 18 Product_1 1532 non-null int64 19 Product_2 1532 non-null int64 20 Product_3 1532 non-null int64 21 Narrative 1532 non-null object 22 Stratum 1532 non-null object 23 PSU 1532 non-null int64 24 Weight 1532 non-null float64 dtypes: float64(6), int64(13), object(6) memory usage: 311.2+ KB . . Reduce Columns . Remove unnecessary columns . # reduce df size and clean na values df2 = df1[[&#39;Treatment_Date&#39;,&#39;Age&#39;,&#39;Sex&#39;,&#39;Body_Part&#39;,&#39;Diagnosis&#39;,&#39;Disposition&#39;,&#39;Location&#39;,&#39;Alcohol&#39;,&#39;Drug&#39;,&#39;Narrative&#39;]].fillna(&#39;&#39;).reset_index(drop=True) df2.head() . Treatment_Date Age Sex Body_Part Diagnosis Disposition Location Alcohol Drug Narrative . 0 1/1/16 | 39 | 1 | 77 | 53 | 1 | 1 | | | 39YOM WAS LIGHTING BOTTLE ROCKETS AND ONE FLEW... | . 1 1/1/16 | 10 | 1 | 82 | 51 | 1 | 1 | | | 10YOM SUSTAINED A THERMAL BURN TO HAND AFTER H... | . 2 1/1/16 | 35 | 1 | 31 | 62 | 4 | 0 | | | 35YOM HIT IN THE CHEST WITH A MORTAR TYPE FIRE... | . 3 1/1/16 | 13 | 1 | 77 | 53 | 1 | 0 | | | 13YOM SOMEONE POINTED FIREWORKS AT HIM FROM 10... | . 4 1/1/16 | 216 | 1 | 31 | 51 | 1 | 0 | | | 16MOM FAMILY PLAYING WITH FIREWORKS AND ONE SH... | . . Fix Sex Data . # Fix Sex columns df2.Sex = df2.Sex.replace(1,&quot;Male&quot;).replace(2,&quot;Female&quot;) df2.head(2) . Treatment_Date Age Sex Body_Part Diagnosis Disposition Location Alcohol Drug Narrative . 0 1/1/16 | 39 | Male | 77 | 53 | 1 | 1 | | | 39YOM WAS LIGHTING BOTTLE ROCKETS AND ONE FLEW... | . 1 1/1/16 | 10 | Male | 82 | 51 | 1 | 1 | | | 10YOM SUSTAINED A THERMAL BURN TO HAND AFTER H... | . . Import Incident Local Dataframe . Dataframe was manual input taken from the 2020 NEISS Coding Manual.pdf . local_df = pd.read_pickle(&#39;df_incident_local.pkl&#39;) local_df . . Code Incident Locale . 0 1 | Home | . 1 2 | Farm/Ranch | . 2 4 | Street or highway | . 3 5 | Other public property | . 4 6 | Manufactured (mobile) home | . 5 7 | Industrial place | . 6 8 | School | . 7 9 | Place of recreation or sports | . 8 0 | Not recorded | . Fix and Mege Incident Locale . # fix Incident Locale df3 = pd.merge(df2, local_df, left_on=&#39;Disposition&#39;, right_on=&#39;Code&#39;).drop([&#39;Code&#39;,&#39;Location&#39;],axis=1) df3.sample(3) . Treatment_Date Age Sex Body_Part Diagnosis Disposition Alcohol Drug Narrative Incident Locale . 1221 5/29/16 | 4 | Female | 31 | 51 | 4 | | | 4YOF W/BURNS TO CHEST &amp; FINGER TIPS 2/2 PLAYIN... | Street or highway | . 541 7/4/18 | 15 | Male | 36 | 72 | 1 | | | 15YOM WAS HIT IN BACK OF LEGS WHEN LARGE FIREW... | Home | . 457 8/2/17 | 25 | Male | 76 | 51 | 1 | | | 25YOM PUT FIREWORKS BOX BON FIRE FIREWORKS WEN... | Home | . . Import Body Part Dataframe . Dataframe was manual input taken from the 2020 NEISS Coding Manual.pdf . body_part_df = pd.read_pickle(&#39;df_body_part.pkl&#39;) body_part_df . . Code Body_Part . 0 0 | Internal | . 1 30 | Shoulder | . 2 31 | Upper Trunk | . 3 32 | Elbow | . 4 33 | Lower Arm | . 5 34 | Wrist | . 6 35 | Knee | . 7 36 | Lower Leg | . 8 37 | Ankle | . 9 38 | Pubic Region | . 10 75 | Head | . 11 76 | Face | . 12 77 | Eyeball | . 13 79 | Lower Trunk | . 14 80 | Upper Arm | . 15 81 | Upper Leg | . 16 82 | Hand | . 17 83 | Foot | . 18 84 | 25-50&#39;%&#39; of Body | . 19 85 | All Parts of Body | . 20 87 | Not Stated | . 21 88 | Mouth | . 22 89 | Neck | . 23 92 | Finger | . 24 93 | Toe | . 25 94 | Ear | . Fix and Merge Body Part Data . # fix Body Part df4 = pd.merge(df3, body_part_df, left_on=&#39;Body_Part&#39;, right_on=&#39;Code&#39;).drop([&#39;Code&#39;,&#39;Body_Part_x&#39;],axis=1) df4.rename(columns={&#39;Body_Part_y&#39;:&#39;Body_Part&#39;},inplace=True) df4.sample(3) . Treatment_Date Age Sex Diagnosis Disposition Alcohol Drug Narrative Incident Locale Body_Part . 169 7/4/20 | 14 | Male | 56 | 1 | 0.0 | 0.0 | 14YOM WAS PLAYING WITH AND WATCHING FIREWORKS ... | Home | Eyeball | . 63 7/5/17 | 32 | Female | 56 | 1 | | | 32YOF W/FOREIGN BODY IN EYE &amp; CONJ IRRITATION ... | Home | Eyeball | . 1153 7/1/20 | 54 | Male | 50 | 2 | 1.0 | 0.0 | 54 YOM PRESENTED TO THE ER WITH A FIREWORK INJ... | Farm/Ranch | Finger | . . Import Diagnosis Dataframe . Dataframe was manual input taken from the 2020 NEISS Coding Manual.pdf . # import Diagnosis pkl diagnosis_df = pd.read_pickle(&#39;df_diagnosis.pkl&#39;) diagnosis_df . . Diagnosis1 Code . 0 Ingested foreign object | 41 | . 1 Aspirated foreign object | 42 | . 2 Burns, electrical 46 Burns, not specified | 47 | . 3 Burns, scald (from hot liquids or steam) | 48 | . 4 Burns, chemical (caustics, etc.) | 49 | . 5 Amputation | 50 | . 6 Burns, thermal (from flames or hot surface) | 51 | . 7 Concussions | 52 | . 8 Contusions, Abrasions | 53 | . 9 Crushing | 54 | . 10 Dislocation | 55 | . 11 Foreign body | 56 | . 12 Fracture | 57 | . 13 Hematoma | 58 | . 14 Laceration | 59 | . 15 Dental injury | 60 | . 16 Nerve damage | 61 | . 17 Internal organ injury | 62 | . 18 Puncture | 63 | . 19 Strain or Sprain | 64 | . 20 Anoxia | 65 | . 21 Hemorrhage | 66 | . 22 Electric shock | 67 | . 23 Poisoning | 68 | . 24 Submersion (including Drowning) | 69 | . 25 Other/Not Stated | 71 | . 26 Avulsion | 72 | . 27 Burns, radiation (includes all cell damage by ... | 73 | . 28 Dermatitis, Conjunctivitis | 74 | . Fix and Merge Diagnosis Data . # fix Diagnosis df5 = pd.merge(df4, diagnosis_df, left_on=&#39;Diagnosis&#39;, right_on=&#39;Code&#39;).drop([&#39;Code&#39;,&#39;Diagnosis&#39;],axis=1) df5.rename(columns={&#39;Diagnosis1&#39;:&#39;Diagnosis&#39;},inplace=True) df5.sample(3) . Treatment_Date Age Sex Disposition Alcohol Drug Narrative Incident Locale Body_Part Diagnosis . 153 6/10/19 | 58 | Female | 1 | 0.0 | 0.0 | 58YOF FALL AND C/O R KNEE, SHOULDER WHEN NEIGH... | Home | Knee | Contusions, Abrasions | . 250 7/2/20 | 32 | Male | 2 | 0.0 | 0.0 | 32 YOM PESENTS WITH EYE INJURY. PT WAS LIGHTIN... | Farm/Ranch | Eyeball | Other/Not Stated | . 783 7/1/16 | 24 | Female | 1 | | | 24 YO FEMALE HURT FINGER ON A TYPE L SPARKLER.... | Home | Finger | Burns, thermal (from flames or hot surface) | . . Import Disposition Dataframe . Dataframe was manual input taken from the 2020 NEISS Coding Manual.pdf . # import Disposition pkl disposition_df = pd.read_pickle(&#39;df_disposition.pkl&#39;) disposition_df . . Code Disposition . 0 1 | Treated/Untreated and Released | . 1 2 | Treated and transferred to another hospital | . 2 3 | Treated and admitted for hospitalization | . 3 4 | Held for observation | . 4 5 | Left without being seen | . 5 6 | Left against medical advice | . 6 7 | Left without treatment | . 7 8 | Eloped Fatality/DOA/died in the ED/Died after ... | . 8 9 | Not recorded | . Fix and Merge Disposition Data . # fix Disposition df6 = pd.merge(df5, disposition_df, left_on=&#39;Disposition&#39;, right_on=&#39;Code&#39;).drop([&#39;Code&#39;,&#39;Disposition_x&#39;],axis=1) df6.rename(columns={&#39;Disposition_y&#39;:&#39;Disposition&#39;},inplace=True) df6.sample(3) . . Treatment_Date Age Sex Alcohol Drug Narrative Incident Locale Body_Part Diagnosis Disposition . 551 7/4/18 | 11 | Female | | | 11YOF WENT TO LIGHT UNKNOWN FIREWORK (TYPE R),... | Home | Upper Leg | Burns, thermal (from flames or hot surface) | Treated/Untreated and Released | . 374 7/8/17 | 212 | Female | | | 12MOF C/O BURN TO R HAND X1 HOUR PTA S/P GRABB... | Home | Hand | Burns, thermal (from flames or hot surface) | Treated/Untreated and Released | . 159 7/18/16 | 12 | Male | | | 12YOM FOREIGN BODY RT EYE WAS PLAYING W/ &quot;BLAS... | Home | Eyeball | Foreign body | Treated/Untreated and Released | . Export Cleaned Data to csv . df6.to_csv(&#39;injury_clean.csv&#39;) .",
            "url": "https://drusho.github.io/blog/blog/pandas/data%20cleaning/python/tableau/2021/06/25/firework-part-1-cleaning.html",
            "relUrl": "/pandas/data%20cleaning/python/tableau/2021/06/25/firework-part-1-cleaning.html",
            "date": " • Jun 25, 2021"
        }
        
    
  

  
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page9": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://drusho.github.io/blog/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}